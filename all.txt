# # from typing import List, Dict, Any, Tuple
# # from moviepy import ImageClip, CompositeVideoClip, TextClip, VideoClip, vfx
# # import numpy as np
# # import os
# # import hashlib
# # import json

# # # ===== Constants =====
# # VIDEO_WIDTH = 1920
# # VIDEO_HEIGHT = 1080
# # FPS = 30
# # BACKGROUND_COLOR = (10, 10, 10)  # RGB

# # # Cache settings
# # CACHE_DIR = "cache/buildshot"
# # os.makedirs(CACHE_DIR, exist_ok=True)

# # PADDING_LEFT = 120
# # PADDING_RIGHT = 120
# # PADDING_TOP = 120
# # PADDING_BOTTOM = 120

# # TEXT_COLOR = (255, 255, 255)
# # TEXT_FONT_SIZE = 80

# # POST_HOLD_SECONDS = 3

# # # Animation settings
# # ANIMATION_DURATION = 0.4  # seconds for smooth repositioning/resize
# # ANIMATION_EASING = "ease_out"  # easing type
# # ENTRANCE_DURATION = 0.45  # seconds for fade-in and translate-in
# # ENTRANCE_TRANSLATE_OFFSET = 28  # pixels to slide in from

# # # Max allowed absolute image upscaling relative to its original/native resolution
# # # This prevents noticeable pixelation when a single/small image would otherwise be blown up
# # MAX_IMAGE_UPSCALE_ABS = 1.5


# # def ease_out_cubic(t: float) -> float:
# #     """Ease-out cubic easing function for smooth animations."""
# #     return 1 - pow(1 - t, 3)


# # def ease_in_out_cubic(t: float) -> float:
# #     """Ease-in-out cubic easing function."""
# #     return 4 * t * t * t if t < 0.5 else 1 - pow(-2 * t + 2, 3) / 2


# # def apply_easing(t: float, easing_type: str = "ease_out") -> float:
# #     """Apply easing function to time value t (0 to 1)."""
# #     t = max(0.0, min(1.0, t))  # clamp to [0, 1]
# #     if easing_type == "ease_out":
# #         return ease_out_cubic(t)
# #     elif easing_type == "ease_in_out":
# #         return ease_in_out_cubic(t)
# #     else:
# #         return t  # linear


# # def _cache_path(media_plan: List[Dict[str, Any]], duration: float) -> str:
# #     """Generate cache path based on media_plan and duration."""
# #     # Create a deterministic hash from the parameters
# #     media_plan_str = json.dumps(media_plan, sort_keys=True, separators=(',', ':'))
# #     key_src = f"{duration}|{media_plan_str}".encode("utf-8")
# #     return os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".mp4")


# # def buildShot(media_plan: List[Dict[str, Any]], duration: float) -> str:
# #     """
# #     Layout + animation engine with dynamic, uniform scaling:
# #       • When few items are visible, they render large.
# #       • As new items appear, all visible items scale down together to fit.
# #       • Position and scale transitions are eased for smooth reflows.
    
# #     Returns:
# #         str: Path to the cached video file
# #     """
# #     if not isinstance(media_plan, list):
# #         raise ValueError("media_plan must be a list")
    
# #     print(f"BuildShot: Processing media_plan with {len(media_plan)} items, duration: {duration}s...")
    
# #     # Check cache first
# #     cache_path = _cache_path(media_plan, duration)
# #     if os.path.exists(cache_path):
# #         print(f"BuildShot: Using cached video: {cache_path}")
# #         return cache_path

# #     # --- Layout bounds ---
# #     available_width = max(1, VIDEO_WIDTH - PADDING_LEFT - PADDING_RIGHT)
# #     available_height = max(1, VIDEO_HEIGHT - PADDING_TOP - PADDING_BOTTOM)

# #     # --- Tunables for layout feel ---
# #     GAP_MIN = 64                # desired minimum horizontal gap between items
# #     MIN_SCALE = 0.35            # never shrink below this
# #     MAX_BASE_W = 2048           # cap initial raster size to avoid huge frames (but allow clean upscales)
# #     MAX_BASE_H = 1536

# #     # Keep only items that actually render
# #     render_items = [
# #         it for it in media_plan
# #         if ("path" in it and it.get("path")) or ("text" in it and it.get("text") is not None)
# #     ]
# #     if not render_items:
# #         # Create empty video if no items to render
# #         from moviepy import ColorClip
# #         empty_clip = ColorClip(size=(VIDEO_WIDTH, VIDEO_HEIGHT), color=BACKGROUND_COLOR, duration=max(0.1, duration))
# #         empty_clip.write_videofile(
# #             cache_path,
# #             fps=FPS,
# #             codec="libx264",
# #             audio=False,
# #             preset="medium",
# #         )
# #         return cache_path

# #     # Build base clips (no time-dependent transforms yet)
# #     # We cap initial size to keep memory sane; dynamic scaling can further scale, but capped.
# #     # Tuple: (clip, appear_at, base_w, base_h, is_image, orig_w, orig_h)
# #     base_clips: List[Tuple[VideoClip, float, int, int, bool, int, int]] = []

# #     for item in render_items:
# #         appear_at = float(item.get("appearAt", 0) or 0)

# #         is_image = bool("path" in item and item.get("path"))

# #         if is_image:
# #             clip = ImageClip(item["path"])
# #         else:
# #             # Text as a clip; start with a generous font size, we'll scale visually over time.
# #             text_color_hex = f"#{TEXT_COLOR[0]:02x}{TEXT_COLOR[1]:02x}{TEXT_COLOR[2]:02x}"
# #             clip = TextClip(
# #                 text=str(item.get("text", "")),
# #                 font_size=TEXT_FONT_SIZE,   # large base; will be dynamically scaled
# #                 color=text_color_hex,
# #                 size=(None, available_height),
# #                 # prevent baseline clipping in MoviePy v2
# #             )

# #         # Constrain the *base* raster size (no upscaling)
# #         orig_w, orig_h = max(1, clip.w), max(1, clip.h)
# #         scale_w = min(1.0, MAX_BASE_W / orig_w)
# #         scale_h = min(1.0, MAX_BASE_H / orig_h)
# #         s = min(scale_w, scale_h)
# #         if s < 1.0:
# #             clip = clip.resized(s)

# #         base_clips.append((clip, appear_at, clip.w, clip.h, is_image, orig_w, orig_h))

# #     # Timeline end: show for given duration or last appear + hold
# #     last_appear = max(a for _, a, _, _, _, _, _ in base_clips) if base_clips else 0.0
# #     final_duration = float(max(duration, last_appear + POST_HOLD_SECONDS))

# #     # Pre-extract arrays for speed
# #     appear_times = [a for _, a, _, _, _, _, _ in base_clips]
# #     widths = [w for _, _, w, _, _, _, _ in base_clips]
# #     heights = [h for _, _, _, h, _, _, _ in base_clips]
# #     is_image_flags = [img for _, _, _, _, img, _, _ in base_clips]
# #     original_widths = [ow for _, _, _, _, _, ow, _ in base_clips]
# #     original_heights = [oh for _, _, _, _, _, _, oh in base_clips]

# #     # --- Shared animation state across all callbacks ---
# #     # Now supports per-item scales so mixed media can share equal widths.
# #     animation_state = {
# #         "last_visible_set": None,          # tuple of indices
# #         "last_positions": {},              # idx -> (x, y) from previous layout
# #         "target_positions": {},            # idx -> (x, y) for current layout
# #         "last_scales": {},                 # idx -> scale for previous layout
# #         "target_scales": {},               # idx -> scale for current layout
# #         "animation_start_time": None,      # global t when layout changed
# #     }

# #     def compute_target_layout(visible: List[int]):
# #         """
# #         Compute per-item scales and positions.
# #         - If one item: scale it up to fill the available area (respecting padding) and center it.
# #         - If multiple items: enforce equal target widths across all items, while respecting height.
# #         Returns: (positions: dict[idx]->(x,y), scales: dict[idx]->float)
# #         """
# #         if not visible:
# #             return {}, {}

# #         n = len(visible)

# #         # Helper: per-item max allowed dynamic scale relative to base raster
# #         # Ensures final display size <= MAX_IMAGE_UPSCALE_ABS * original resolution
# #         def allowed_scale_limit(idx: int) -> float:
# #             if not is_image_flags[idx]:
# #                 return float("inf")  # no hard cap for text
# #             base_w = max(1.0, widths[idx])
# #             base_h = max(1.0, heights[idx])
# #             o_w = max(1.0, original_widths[idx])
# #             o_h = max(1.0, original_heights[idx])
# #             # Aspect ratio preserved; width ratio == height ratio == (final/base)
# #             limit_w = MAX_IMAGE_UPSCALE_ABS * (o_w / base_w)
# #             limit_h = MAX_IMAGE_UPSCALE_ABS * (o_h / base_h)
# #             return max(MIN_SCALE, min(limit_w, limit_h))

# #         # Single item: maximize within available rect (allow upscaling but capped for images)
# #         if n == 1:
# #             idx = visible[0]
# #             s_fill_w = available_width / max(1, widths[idx])
# #             s_fill_h = available_height / max(1, heights[idx])
# #             s = max(MIN_SCALE, min(s_fill_w, s_fill_h))
# #             # Cap image upscaling to avoid pixelation
# #             s = min(s, allowed_scale_limit(idx))

# #             scaled_w = widths[idx] * s
# #             scaled_h = heights[idx] * s
# #             x_left = int(PADDING_LEFT + (available_width - scaled_w) / 2)
# #             y_top = int(PADDING_TOP + (available_height - scaled_h) / 2)
# #             return {idx: (x_left, y_top)}, {idx: s}

# #         # Multiple items: equal width for all, with adaptive gaps
# #         content_width_budget = max(1.0, available_width - GAP_MIN * (n + 1))
# #         equal_target_width = content_width_budget / n

# #         # Also respect image caps: equal target width cannot exceed any item's allowed max width
# #         max_width_per_item = []
# #         for i in visible:
# #             cap_scale = allowed_scale_limit(i)
# #             max_width_per_item.append(widths[i] * cap_scale)
# #         if max_width_per_item:
# #             equal_target_width = min(equal_target_width, min(max_width_per_item))

# #         # Initial per-item scales to reach equal width
# #         raw_scales = {i: equal_target_width / max(1.0, widths[i]) for i in visible}
# #         # Ensure we never go below MIN_SCALE and never exceed per-item cap
# #         for i in visible:
# #             raw_scales[i] = max(MIN_SCALE, min(raw_scales[i], allowed_scale_limit(i)))

# #         # If heights overflow, shrink all uniformly
# #         scaled_heights = [heights[i] * raw_scales[i] for i in visible]
# #         max_scaled_h = max(scaled_heights) if scaled_heights else 0.0
# #         if max_scaled_h > available_height:
# #             height_scale = available_height / max_scaled_h
# #             for i in visible:
# #                 raw_scales[i] *= height_scale

# #         # With resulting scales, compute actual free width to distribute as gaps
# #         scaled_sum_w = sum(widths[i] * raw_scales[i] for i in visible)
# #         free_w = max(0.0, available_width - scaled_sum_w)
# #         gap = free_w / (n + 1)

# #         # Positions: horizontally spaced with equal gaps, vertically centered per item height
# #         positions = {}
# #         x_cursor = PADDING_LEFT + gap
# #         for i in visible:
# #             scaled_h = heights[i] * raw_scales[i]
# #             y_top = int(PADDING_TOP + (available_height - scaled_h) / 2)
# #             positions[i] = (int(x_cursor), y_top)
# #             x_cursor += widths[i] * raw_scales[i] + gap

# #         return positions, raw_scales

# #     def ensure_layout_for_time(global_t: float):
# #         """
# #         Update target layout if the set of visible clips has changed.
# #         Start a new reflow animation window when it does.
# #         """
# #         visible = [i for i, a in enumerate(appear_times) if a <= global_t]
# #         visible_set = tuple(sorted(visible))

# #         if animation_state["last_visible_set"] != visible_set:
# #             new_positions, new_scales = compute_target_layout(visible)
# #             animation_state["target_positions"] = new_positions
# #             animation_state["target_scales"] = new_scales

# #             # Initialize or carry previous layout for smooth interpolation
# #             if animation_state["last_visible_set"] is None:
# #                 animation_state["last_positions"] = new_positions.copy()
# #                 animation_state["last_scales"] = new_scales.copy()
# #             else:
# #                 # Ensure every newly-visible clip has a starting pos/scale (use target as fallback)
# #                 for idx in visible:
# #                     if idx not in animation_state["last_positions"]:
# #                         animation_state["last_positions"][idx] = new_positions[idx]
# #                     if idx not in animation_state["last_scales"]:
# #                         animation_state["last_scales"][idx] = new_scales[idx]

# #             animation_state["animation_start_time"] = global_t
# #             animation_state["last_visible_set"] = visible_set

# #     def current_interp(global_t: float):
# #         """
# #         Ensure layout is up-to-date and return eased interpolation factor (0..1).
# #         Also commits target state when the animation window ends.
# #         """
# #         ensure_layout_for_time(global_t)

# #         in_anim = (
# #             animation_state["animation_start_time"] is not None and
# #             global_t < animation_state["animation_start_time"] + ANIMATION_DURATION
# #         )
# #         if not in_anim:
# #             # Lock in the target layout after animation window ends
# #             animation_state["last_positions"] = animation_state["target_positions"].copy()
# #             animation_state["last_scales"] = animation_state["target_scales"].copy()
# #             return 1.0

# #         # Progress (0..1) eased
# #         raw = (global_t - animation_state["animation_start_time"]) / ANIMATION_DURATION
# #         eased = apply_easing(max(0.0, min(1.0, raw)), ANIMATION_EASING)
# #         return eased

# #     def make_position_fn(self_idx: int, own_appear_time: float):
# #         def pos(local_t: float):
# #             # Convert to absolute timeline (global time)
# #             local_t = max(0.0, local_t)
# #             global_t = own_appear_time + local_t

# #             # Visibility gate
# #             if global_t < own_appear_time:
# #                 return (-99999, -99999)

# #             eased = current_interp(global_t)

# #             # Interpolate this clip's position
# #             last_pos = animation_state["last_positions"].get(self_idx)
# #             target_pos = animation_state["target_positions"].get(self_idx)

# #             if last_pos is None or target_pos is None:
# #                 return (-99999, -99999)

# #             x0, y0 = last_pos
# #             x1, y1 = target_pos
# #             cx = int(x0 + (x1 - x0) * eased)
# #             cy = int(y0 + (y1 - y0) * eased)

# #             # Entrance slide-in for this clip only (local time window)
# #             if local_t < ENTRANCE_DURATION:
# #                 p = apply_easing(local_t / ENTRANCE_DURATION, "ease_out")
# #                 offset = int(ENTRANCE_TRANSLATE_OFFSET * (1 - p))
# #                 cx += offset

# #             return (cx, cy)
# #         return pos

# #     def make_scale_fn(self_idx: int, own_appear_time: float):
# #         def scale(local_t: float):
# #             local_t = max(0.0, local_t)
# #             global_t = own_appear_time + local_t

# #             # If not yet visible, return something (won't be sampled visually anyway)
# #             if global_t < own_appear_time:
# #                 return animation_state["last_scales"].get(self_idx, animation_state["target_scales"].get(self_idx, 1.0))

# #             eased = current_interp(global_t)

# #             # Interpolate this clip's scale between last and target per-item scales
# #             s0 = animation_state["last_scales"].get(self_idx)
# #             s1 = animation_state["target_scales"].get(self_idx)
# #             if s0 is None or s1 is None:
# #                 # Fallback to target scale if something is missing
# #                 s = s1 if s1 is not None else 1.0
# #             else:
# #                 s = s0 + (s1 - s0) * eased

# #             # Tiny entrance emphasis
# #             if local_t < ENTRANCE_DURATION:
# #                 p = apply_easing(local_t / ENTRANCE_DURATION, "ease_out")
# #                 s = s * (0.985 + 0.015 * p)

# #             return s
# #         return scale

# #     # Compose clips with dynamic position + scale + fade-in
# #     composed = []
# #     for idx, (clip, appear_at, _, _, _, _, _) in enumerate(base_clips):
# #         clip_duration = max(0.01, final_duration - appear_at)
# #         fade_dur = min(ENTRANCE_DURATION, clip_duration)

# #         animated_clip = (
# #             clip
# #             .with_start(appear_at)
# #             .with_duration(clip_duration)
# #             .with_position(make_position_fn(idx, appear_at))
# #             .with_effects([
# #                 vfx.Resize(make_scale_fn(idx, appear_at)),   # dynamic, uniform scaling
# #                 vfx.CrossFadeIn(fade_dur),                   # entrance fade
# #             ])
# #         )
# #         composed.append(animated_clip)

# #     # Render
# #     print(f"BuildShot: Generating new video: {cache_path}")
# #     CompositeVideoClip(
# #         composed,
# #         size=(VIDEO_WIDTH, VIDEO_HEIGHT),
# #         bg_color=BACKGROUND_COLOR,
# #     ).with_duration(final_duration).write_videofile(
# #         cache_path,
# #         fps=FPS,
# #         codec="libx264",
# #         audio=False,
# #         preset="medium",
# #     )
    
# #     return cache_path

# from typing import List, Dict, Any, Tuple
# from moviepy import ImageClip, CompositeVideoClip, TextClip, VideoClip, vfx
# import numpy as np
# import os
# import hashlib
# import json

# # ===== Constants =====
# VIDEO_WIDTH = 1920
# VIDEO_HEIGHT = 1080
# FPS = 30
# BACKGROUND_COLOR = (10, 10, 10)  # RGB

# # Layout controls
# MAX_ITEMS_PER_ROW = 2           # 👈 cap items per row; overflow wraps to the next row
# H_GAP_MIN = 64                  # minimum horizontal gap inside a row
# V_GAP_MIN = 56                  # minimum vertical gap between rows

# # Cache settings
# CACHE_DIR = "cache/buildshot"
# os.makedirs(CACHE_DIR, exist_ok=True)

# PADDING_LEFT = 120
# PADDING_RIGHT = 120
# PADDING_TOP = 120
# PADDING_BOTTOM = 120

# TEXT_COLOR = (255, 255, 255)
# TEXT_FONT_SIZE = 80

# POST_HOLD_SECONDS = 3

# # Animation settings
# ANIMATION_DURATION = 0.4   # seconds for smooth repositioning/resize
# ANIMATION_EASING = "ease_out"  # easing type
# ENTRANCE_DURATION = 0.45   # seconds for fade-in and translate-in
# ENTRANCE_TRANSLATE_OFFSET = 28  # pixels to slide in from

# # Max allowed absolute image upscaling relative to its original/native resolution
# # This prevents noticeable pixelation when a single/small image would otherwise be blown up
# MAX_IMAGE_UPSCALE_ABS = 1.5


# def ease_out_cubic(t: float) -> float:
#     """Ease-out cubic easing function for smooth animations."""
#     return 1 - pow(1 - t, 3)


# def ease_in_out_cubic(t: float) -> float:
#     """Ease-in-out cubic easing function."""
#     return 4 * t * t * t if t < 0.5 else 1 - pow(-2 * t + 2, 3) / 2


# def apply_easing(t: float, easing_type: str = "ease_out") -> float:
#     """Apply easing function to time value t (0 to 1)."""
#     t = max(0.0, min(1.0, t))  # clamp to [0, 1]
#     if easing_type == "ease_out":
#         return ease_out_cubic(t)
#     elif easing_type == "ease_in_out":
#         return ease_in_out_cubic(t)
#     else:
#         return t  # linear


# def _cache_path(media_plan: List[Dict[str, Any]], duration: float) -> str:
#     """Generate cache path based on media_plan and duration."""
#     media_plan_str = json.dumps(media_plan, sort_keys=True, separators=(',', ':'))
#     key_src = f"{duration}|{media_plan_str}".encode("utf-8")
#     return os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".mp4")


# def buildShot(media_plan: List[Dict[str, Any]], duration: float) -> str:
#     """
#     Layout + animation engine with dynamic, uniform scaling and row wrapping:
#       • When few items are visible, they render large.
#       • As new items appear, all visible items scale and reflow smoothly.
#       • Items wrap to additional rows when MAX_ITEMS_PER_ROW is reached.
#       • Rows and items are perfectly centered with even spacing.
#     Returns:
#         str: Path to the cached video file
#     """
#     if not isinstance(media_plan, list):
#         raise ValueError("media_plan must be a list")

#     print(f"BuildShot: Processing media_plan with {len(media_plan)} items, duration: {duration}s...")

#     # Check cache first
#     cache_path = _cache_path(media_plan, duration)
#     if os.path.exists(cache_path):
#         print(f"BuildShot: Using cached video: {cache_path}")
#         return cache_path

#     # --- Layout bounds ---
#     available_width = max(1, VIDEO_WIDTH - PADDING_LEFT - PADDING_RIGHT)
#     available_height = max(1, VIDEO_HEIGHT - PADDING_TOP - PADDING_BOTTOM)

#     # --- Tunables for layout feel ---
#     MIN_SCALE = 0.35            # never shrink below this before global fit checks
#     MAX_BASE_W = 2048           # cap initial raster size to avoid huge frames (but allow clean upscales)
#     MAX_BASE_H = 1536

#     # Keep only items that actually render
#     render_items = [
#         it for it in media_plan
#         if ("path" in it and it.get("path")) or ("text" in it and it.get("text") is not None)
#     ]
#     if not render_items:
#         from moviepy import ColorClip
#         empty_clip = ColorClip(size=(VIDEO_WIDTH, VIDEO_HEIGHT), color=BACKGROUND_COLOR, duration=max(0.1, duration))
#         empty_clip.write_videofile(
#             cache_path,
#             fps=FPS,
#             codec="libx264",
#             audio=False,
#             preset="medium",
#         )
#         return cache_path

#     # Build base clips (no time-dependent transforms yet)
#     # Tuple: (clip, appear_at, base_w, base_h, is_image, orig_w, orig_h)
#     base_clips: List[Tuple[VideoClip, float, int, int, bool, int, int]] = []

#     for item in render_items:
#         appear_at = float(item.get("appearAt", 0) or 0)
#         is_image = bool("path" in item and item.get("path"))

#         if is_image:
#             clip = ImageClip(item["path"])
#         else:
#             text_color_hex = f"#{TEXT_COLOR[0]:02x}{TEXT_COLOR[1]:02x}{TEXT_COLOR[2]:02x}"
#             clip = TextClip(
#                 text=str(item.get("text", "")),
#                 font_size=TEXT_FONT_SIZE,
#                 color=text_color_hex,
#                 size=(None, available_height),
#             )

#         # Constrain the *base* raster size (no upscaling)
#         orig_w, orig_h = max(1, clip.w), max(1, clip.h)
#         scale_w = min(1.0, MAX_BASE_W / orig_w)
#         scale_h = min(1.0, MAX_BASE_H / orig_h)
#         s = min(scale_w, scale_h)
#         if s < 1.0:
#             clip = clip.resized(s)

#         base_clips.append((clip, appear_at, clip.w, clip.h, is_image, orig_w, orig_h))

#     # Timeline end: show for given duration or last appear + hold
#     last_appear = max(a for _, a, _, _, _, _, _ in base_clips) if base_clips else 0.0
#     final_duration = float(max(duration, last_appear + POST_HOLD_SECONDS))

#     # Pre-extract arrays for speed
#     appear_times = [a for _, a, _, _, _, _, _ in base_clips]
#     widths = [w for _, _, w, _, _, _, _ in base_clips]
#     heights = [h for _, _, _, h, _, _, _ in base_clips]
#     is_image_flags = [img for _, _, _, _, img, _, _ in base_clips]
#     original_widths = [ow for _, _, _, _, _, ow, _ in base_clips]
#     original_heights = [oh for _, _, _, _, _, _, oh in base_clips]

#     # --- Shared animation state across all callbacks ---
#     animation_state = {
#         "last_visible_set": None,          # tuple of indices
#         "last_positions": {},              # idx -> (x, y) from previous layout
#         "target_positions": {},            # idx -> (x, y) for current layout
#         "last_scales": {},                 # idx -> scale for previous layout
#         "target_scales": {},               # idx -> scale for current layout
#         "animation_start_time": None,      # global t when layout changed
#     }

#     def compute_target_layout(visible: List[int]):
#         """
#         Compute per-item scales and positions with row wrapping.
#         - Items are split into rows of at most MAX_ITEMS_PER_ROW.
#         - Within each row, items target equal widths; rows are centered and evenly spaced.
#         - If overall content exceeds available height, all rows/items shrink uniformly.
#         Returns: (positions: dict[idx]->(x,y), scales: dict[idx]->float)
#         """
#         if not visible:
#             return {}, {}

#         # Helper: per-item max allowed dynamic scale relative to base raster
#         def allowed_scale_limit(idx: int) -> float:
#             if not is_image_flags[idx]:
#                 return float("inf")  # no hard cap for text
#             base_w = max(1.0, widths[idx])
#             base_h = max(1.0, heights[idx])
#             o_w = max(1.0, original_widths[idx])
#             o_h = max(1.0, original_heights[idx])
#             limit_w = MAX_IMAGE_UPSCALE_ABS * (o_w / base_w)
#             limit_h = MAX_IMAGE_UPSCALE_ABS * (o_h / base_h)
#             return max(MIN_SCALE, min(limit_w, limit_h))

#         # Split into rows
#         rows: List[List[int]] = []
#         if MAX_ITEMS_PER_ROW <= 0:
#             rows.append(list(visible))
#         else:
#             for i in range(0, len(visible), MAX_ITEMS_PER_ROW):
#                 rows.append(visible[i:i + MAX_ITEMS_PER_ROW])

#         # First pass: compute raw per-item scales per row (equal target width inside a row)
#         scales: Dict[int, float] = {}
#         row_heights: List[float] = []

#         for row in rows:
#             n = len(row)
#             if n == 0:
#                 row_heights.append(0.0)
#                 continue

#             content_width_budget = max(1.0, available_width - H_GAP_MIN * (n + 1))
#             equal_target_width = content_width_budget / n

#             # Ensure equal width doesn't exceed any item's allowed cap
#             max_width_per_item = []
#             for idx in row:
#                 cap_scale = allowed_scale_limit(idx)
#                 max_width_per_item.append(widths[idx] * cap_scale)
#             if max_width_per_item:
#                 equal_target_width = min(equal_target_width, min(max_width_per_item))

#             # Per-item scale to reach equal width, clamped to limits
#             for idx in row:
#                 s = equal_target_width / max(1.0, widths[idx])
#                 s = max(MIN_SCALE, min(s, allowed_scale_limit(idx)))
#                 scales[idx] = s

#             # Row height is the max scaled height in the row
#             row_heights.append(max(heights[idx] * scales[idx] for idx in row))

#         # Second pass: ensure total vertical fit (rows + gaps) within available height
#         total_rows = len(rows)
#         total_content_height = sum(row_heights) + V_GAP_MIN * max(0, total_rows - 1)
#         if total_content_height > available_height:
#             # Uniformly scale down all items to fit vertically
#             shrink = available_height / total_content_height
#             for idx in scales:
#                 scales[idx] *= shrink
#             row_heights = [h * shrink for h in row_heights]
#             total_content_height = sum(row_heights) + V_GAP_MIN * max(0, total_rows - 1)

#         # Third pass: compute positions (center block vertically; center items in rows with even gaps)
#         positions: Dict[int, Tuple[int, int]] = {}
#         y_cursor = PADDING_TOP + (available_height - total_content_height) / 2.0

#         for r, row in enumerate(rows):
#             n = len(row)
#             if n == 0:
#                 continue

#             # Row width after scaling
#             row_scaled_width = sum(widths[idx] * scales[idx] for idx in row)
#             free_w = max(0.0, available_width - row_scaled_width)
#             gap = free_w / (n + 1)

#             x_cursor = PADDING_LEFT + gap
#             row_h = row_heights[r]

#             for idx in row:
#                 scaled_w = widths[idx] * scales[idx]
#                 scaled_h = heights[idx] * scales[idx]
#                 # Vertically center each item inside the row band
#                 y_top = int(y_cursor + (row_h - scaled_h) / 2.0)
#                 positions[idx] = (int(x_cursor), y_top)
#                 x_cursor += scaled_w + gap

#             y_cursor += row_h + (V_GAP_MIN if r < total_rows - 1 else 0.0)

#         return positions, scales

#     def ensure_layout_for_time(global_t: float):
#         """
#         Update target layout if the set of visible clips has changed.
#         Start a new reflow animation window when it does.
#         """
#         visible = [i for i, a in enumerate(appear_times) if a <= global_t]
#         visible_set = tuple(sorted(visible))

#         if animation_state["last_visible_set"] != visible_set:
#             new_positions, new_scales = compute_target_layout(visible)
#             animation_state["target_positions"] = new_positions
#             animation_state["target_scales"] = new_scales

#             # Initialize or carry previous layout for smooth interpolation
#             if animation_state["last_visible_set"] is None:
#                 animation_state["last_positions"] = new_positions.copy()
#                 animation_state["last_scales"] = new_scales.copy()
#             else:
#                 for idx in visible:
#                     if idx not in animation_state["last_positions"]:
#                         animation_state["last_positions"][idx] = new_positions[idx]
#                     if idx not in animation_state["last_scales"]:
#                         animation_state["last_scales"][idx] = new_scales[idx]

#             animation_state["animation_start_time"] = global_t
#             animation_state["last_visible_set"] = visible_set

#     def current_interp(global_t: float):
#         """
#         Ensure layout is up-to-date and return eased interpolation factor (0..1).
#         Also commits target state when the animation window ends.
#         """
#         ensure_layout_for_time(global_t)

#         in_anim = (
#             animation_state["animation_start_time"] is not None and
#             global_t < animation_state["animation_start_time"] + ANIMATION_DURATION
#         )
#         if not in_anim:
#             # Lock in the target layout after animation window ends
#             animation_state["last_positions"] = animation_state["target_positions"].copy()
#             animation_state["last_scales"] = animation_state["target_scales"].copy()
#             return 1.0

#         raw = (global_t - animation_state["animation_start_time"]) / ANIMATION_DURATION
#         eased = apply_easing(max(0.0, min(1.0, raw)), ANIMATION_EASING)
#         return eased

#     def make_position_fn(self_idx: int, own_appear_time: float):
#         def pos(local_t: float):
#             local_t = max(0.0, local_t)
#             global_t = own_appear_time + local_t

#             eased = current_interp(global_t)

#             last_pos = animation_state["last_positions"].get(self_idx)
#             target_pos = animation_state["target_positions"].get(self_idx)
#             if last_pos is None or target_pos is None:
#                 return (-99999, -99999)

#             x0, y0 = last_pos
#             x1, y1 = target_pos
#             cx = int(x0 + (x1 - x0) * eased)
#             cy = int(y0 + (y1 - y0) * eased)

#             # Entrance slide-in for this clip only (local time window)
#             if local_t < ENTRANCE_DURATION:
#                 p = apply_easing(local_t / ENTRANCE_DURATION, "ease_out")
#                 offset = int(ENTRANCE_TRANSLATE_OFFSET * (1 - p))
#                 cx += offset

#             return (cx, cy)
#         return pos

#     def make_scale_fn(self_idx: int, own_appear_time: float):
#         def scale(local_t: float):
#             local_t = max(0.0, local_t)
#             global_t = own_appear_time + local_t

#             eased = current_interp(global_t)

#             s0 = animation_state["last_scales"].get(self_idx)
#             s1 = animation_state["target_scales"].get(self_idx)
#             if s0 is None or s1 is None:
#                 s = s1 if s1 is not None else 1.0
#             else:
#                 s = s0 + (s1 - s0) * eased

#             # Tiny entrance emphasis
#             if local_t < ENTRANCE_DURATION:
#                 p = apply_easing(local_t / ENTRANCE_DURATION, "ease_out")
#                 s = s * (0.985 + 0.015 * p)

#             return s
#         return scale

#     # Compose clips with dynamic position + scale + fade-in
#     composed = []
#     for idx, (clip, appear_at, _, _, _, _, _) in enumerate(base_clips):
#         clip_duration = max(0.01, final_duration - appear_at)
#         fade_dur = min(ENTRANCE_DURATION, clip_duration)

#         animated_clip = (
#             clip
#             .with_start(appear_at)
#             .with_duration(clip_duration)
#             .with_position(make_position_fn(idx, appear_at))
#             .with_effects([
#                 vfx.Resize(make_scale_fn(idx, appear_at)),   # dynamic, uniform scaling
#                 vfx.CrossFadeIn(fade_dur),                   # entrance fade
#             ])
#         )
#         composed.append(animated_clip)

#     # Render
#     print(f"BuildShot: Generating new video: {cache_path}")
#     CompositeVideoClip(
#         composed,
#         size=(VIDEO_WIDTH, VIDEO_HEIGHT),
#         bg_color=BACKGROUND_COLOR,
#     ).with_duration(final_duration).write_videofile(
#         cache_path,
#         fps=FPS,
#         codec="libx264",
#         audio=False,
#         preset="medium",
#     )

#     return cache_path

# from typing import List, Dict, Any, Tuple
# from moviepy import ImageClip, CompositeVideoClip, TextClip, VideoClip, vfx
# import os
# import hashlib
# import json

# # ===== Constants =====
# VIDEO_WIDTH = 1920
# VIDEO_HEIGHT = 1080
# FPS = 30
# BACKGROUND_COLOR = (10, 10, 10)  # RGB

# # Layout controls
# MAX_ITEMS_PER_ROW = 2            # 👈 cap items per row; overflow wraps to next row(s)
# H_GAP_MIN = 64                   # minimum horizontal gap inside a row
# V_GAP_MIN = 16                   # minimum vertical gap between rows
# ROW_HEIGHT_DECAY = 0.3           # 👈 bottom row gets most height; each row above has this fraction of the row below

# # Cache settings
# CACHE_DIR = "cache/buildshot"
# os.makedirs(CACHE_DIR, exist_ok=True)

# PADDING_LEFT = 120
# PADDING_RIGHT = 120
# PADDING_TOP = 120
# PADDING_BOTTOM = 120

# TEXT_COLOR = (0, 0, 0)
# TEXT_FONT_SIZE = 80
# FONT_FILE = "font.ttf"
# BACKGROUND_IMAGE = "background.png"

# POST_HOLD_SECONDS = 3

# # Animation settings
# ANIMATION_DURATION = 0.4         # seconds for smooth repositioning/resize
# ANIMATION_EASING = "ease_out"    # easing type
# ENTRANCE_DURATION = 0.45         # seconds for fade-in and translate-in
# ENTRANCE_TRANSLATE_OFFSET = 28   # pixels to slide in from

# # Group near-simultaneous arrivals to reduce layout thrashing/jitter
# COALESCE_LOOKAHEAD = 0.25        # seconds to look ahead when computing layout

# # Max allowed absolute image upscaling relative to its original/native resolution
# # This prevents noticeable pixelation when a single/small image would otherwise be blown up
# MAX_IMAGE_UPSCALE_ABS = 1.5


# def ease_out_cubic(t: float) -> float:
#     return 1 - pow(1 - t, 3)


# def ease_in_out_cubic(t: float) -> float:
#     return 4 * t * t * t if t < 0.5 else 1 - pow(-2 * t + 2, 3) / 2


# def apply_easing(t: float, easing_type: str = "ease_out") -> float:
#     t = max(0.0, min(1.0, t))
#     if easing_type == "ease_out":
#         return ease_out_cubic(t)
#     elif easing_type == "ease_in_out":
#         return ease_in_out_cubic(t)
#     return t


# def _cache_path(media_plan: List[Dict[str, Any]], duration: float) -> str:
#     media_plan_str = json.dumps(media_plan, sort_keys=True, separators=(',', ':'))
#     key_src = f"{duration}|{media_plan_str}".encode("utf-8")
#     return os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".mp4")


# def buildShot(media_plan: List[Dict[str, Any]], duration: float) -> str:
#     """
#     Layout + animation engine with dynamic scaling, row wrapping, and prioritized bottom-row height:
#       • Items wrap when MAX_ITEMS_PER_ROW is reached.
#       • Bottom row takes the majority of vertical space; rows above receive progressively less (ROW_HEIGHT_DECAY).
#       • Items in each row have equal target widths, are horizontally centered with even gaps, and vertically centered within their row band.
#       • Smooth eased transitions on reflow and entrance.
#     Returns:
#         str: Path to the cached video file
#     """
#     if not isinstance(media_plan, list):
#         raise ValueError("media_plan must be a list")

#     cache_path = _cache_path(media_plan, duration)
#     if os.path.exists(cache_path):
#         return cache_path

#     available_width = max(1, VIDEO_WIDTH - PADDING_LEFT - PADDING_RIGHT)
#     available_height = max(1, VIDEO_HEIGHT - PADDING_TOP - PADDING_BOTTOM)

#     # Tunables
#     MIN_SCALE = 0.35
#     MAX_BASE_W = 2048
#     MAX_BASE_H = 1536

#     # Filter items that actually render
#     render_items = [
#         it for it in media_plan
#         if ("path" in it and it.get("path")) or ("text" in it and it.get("text") is not None)
#     ]
#     if not render_items:
#         from moviepy import ColorClip
#         ColorClip(size=(VIDEO_WIDTH, VIDEO_HEIGHT), color=BACKGROUND_COLOR, duration=max(0.1, duration)).write_videofile(
#             cache_path, fps=FPS, codec="libx264", audio=False, preset="medium"
#         )
#         return cache_path

#     # Build base clips
#     base_clips: List[Tuple[VideoClip, float, int, int, bool, int, int]] = []
#     for item in render_items:
#         appear_at = float(item.get("appearAt", 0) or 0)
#         is_image = bool("path" in item and item.get("path"))

#         if is_image:
#             clip = ImageClip(item["path"])
#         else:
#             text_color_hex = f"#{TEXT_COLOR[0]:02x}{TEXT_COLOR[1]:02x}{TEXT_COLOR[2]:02x}"
#             clip = TextClip(
#                 text=str(item.get("text", "")),
#                 font_size=TEXT_FONT_SIZE,
#                 color=text_color_hex,
#                 size=(None, available_height),
#                 font=FONT_FILE,
#             )

#         orig_w, orig_h = max(1, clip.w), max(1, clip.h)
#         s = min(1.0, MAX_BASE_W / orig_w, MAX_BASE_H / orig_h)
#         if s < 1.0:
#             clip = clip.resized(s)

#         base_clips.append((clip, appear_at, clip.w, clip.h, is_image, orig_w, orig_h))

#     last_appear = max(a for _, a, _, _, _, _, _ in base_clips) if base_clips else 0.0
#     final_duration = float(max(duration, last_appear + POST_HOLD_SECONDS))

#     appear_times = [a for _, a, _, _, _, _, _ in base_clips]
#     widths = [w for _, _, w, _, _, _, _ in base_clips]
#     heights = [h for _, _, _, h, _, _, _ in base_clips]
#     is_image_flags = [img for _, _, _, _, img, _, _ in base_clips]
#     original_widths = [ow for _, _, _, _, _, ow, _ in base_clips]
#     original_heights = [oh for _, _, _, _, _, _, oh in base_clips]

#     # Animation state
#     animation_state = {
#         "last_visible_set": None,
#         "last_positions": {},
#         "target_positions": {},
#         "last_scales": {},
#         "target_scales": {},
#         "animation_start_time": None,
#     }

#     def allowed_scale_limit(idx: int) -> float:
#         if not is_image_flags[idx]:
#             return float("inf")
#         base_w = max(1.0, widths[idx])
#         base_h = max(1.0, heights[idx])
#         o_w = max(1.0, original_widths[idx])
#         o_h = max(1.0, original_heights[idx])
#         limit_w = MAX_IMAGE_UPSCALE_ABS * (o_w / base_w)
#         limit_h = MAX_IMAGE_UPSCALE_ABS * (o_h / base_h)
#         return max(MIN_SCALE, min(limit_w, limit_h))

#     def compute_target_layout(visible: List[int]):
#         if not visible:
#             return {}, {}

#         # Split into rows by MAX_ITEMS_PER_ROW
#         rows: List[List[int]] = []
#         if MAX_ITEMS_PER_ROW <= 0:
#             rows.append(list(visible))
#         else:
#             for i in range(0, len(visible), MAX_ITEMS_PER_ROW):
#                 rows.append(visible[i:i + MAX_ITEMS_PER_ROW])

#         num_rows = len(rows)

#         # Compute row weights so bottom row has weight 1.0 and each row above decays
#         # Example (3 rows): top=decay^2, mid=decay^1, bottom=1.0
#         weights = [ROW_HEIGHT_DECAY ** (num_rows - 1 - r) for r in range(num_rows)]
#         weight_sum = max(1e-6, sum(weights))

#         # Total vertical gap space
#         total_gap_h = V_GAP_MIN * max(0, num_rows - 1)
#         band_total_height = max(1.0, available_height - total_gap_h)

#         # Row band heights from weights (bottom gets the largest share)
#         row_bands: List[float] = [band_total_height * (w / weight_sum) for w in weights]

#         # Per-item scales and positions to fill bands, preserving equal widths inside each row
#         scales: Dict[int, float] = {}
#         row_heights: List[float] = []

#         for r, row in enumerate(rows):
#             n = len(row)
#             if n == 0:
#                 row_heights.append(0.0)
#                 continue

#             # Width budget for content (keeping at least H_GAP_MIN gaps)
#             content_width_budget = max(1.0, available_width - H_GAP_MIN * (n + 1))
#             equal_target_width = content_width_budget / n

#             # Cap equal target width by each item's upscale limit
#             max_width_per_item = [widths[idx] * allowed_scale_limit(idx) for idx in row]
#             if max_width_per_item:
#                 equal_target_width = min(equal_target_width, min(max_width_per_item))

#             # Initial per-item scales to hit equal target width
#             for idx in row:
#                 s_i = equal_target_width / max(1.0, widths[idx])
#                 s_i = min(s_i, allowed_scale_limit(idx))
#                 scales[idx] = max(1e-6, s_i)  # avoid zero/neg

#             # Enforce band height: shrink uniformly within this row if needed
#             max_scaled_h = max(heights[idx] * scales[idx] for idx in row)
#             band_h = row_bands[r]
#             if max_scaled_h > band_h:
#                 shrink = band_h / max_scaled_h
#                 for idx in row:
#                     scales[idx] *= shrink

#             # If row still exceeds horizontal width (numerical edge cases), shrink uniformly
#             row_scaled_w = sum(widths[idx] * scales[idx] for idx in row)
#             if row_scaled_w > available_width:
#                 shrink_w = available_width / row_scaled_w
#                 for idx in row:
#                     scales[idx] *= shrink_w

#             # Record actual row height used by tallest item after adjustments
#             row_heights.append(max(heights[idx] * scales[idx] for idx in row))

#         # Positions: allocate vertical bands (rows + fixed gaps fill the available height exactly)
#         positions: Dict[int, Tuple[int, int]] = {}
#         y_cursor = PADDING_TOP
#         for r, row in enumerate(rows):
#             n = len(row)
#             if n == 0:
#                 continue

#             # Horizontal layout: compute gap so items are evenly spaced and centered
#             row_scaled_width = sum(widths[idx] * scales[idx] for idx in row)
#             free_w = max(0.0, available_width - row_scaled_width)
#             gap_x = free_w / (n + 1)

#             x_cursor = PADDING_LEFT + gap_x
#             # Vertically center each item inside its row band
#             band_h = row_bands[r]
#             for idx in row:
#                 scaled_w = widths[idx] * scales[idx]
#                 scaled_h = heights[idx] * scales[idx]
#                 y_top = int(y_cursor + (band_h - scaled_h) / 2.0)
#                 positions[idx] = (int(x_cursor), y_top)
#                 x_cursor += scaled_w + gap_x

#             # Advance to next row band (add fixed vertical gap between rows)
#             y_cursor += band_h + (V_GAP_MIN if r < num_rows - 1 else 0.0)

#         return positions, scales

#     def ensure_layout_for_time(global_t: float):
#         # Coalesce items that arrive within a small lookahead window to avoid back-to-back reflows
#         planned_visible = [i for i, a in enumerate(appear_times) if a <= global_t + COALESCE_LOOKAHEAD]
#         planned_set = tuple(sorted(planned_visible))

#         if animation_state["last_visible_set"] != planned_set:
#             # Compute the current, in-flight position/scale as the new baseline to prevent jumps
#             prev_start = animation_state.get("animation_start_time")
#             if prev_start is not None and animation_state["target_positions"]:
#                 prev_raw = (global_t - prev_start) / ANIMATION_DURATION
#                 prev_eased = apply_easing(max(0.0, min(1.0, prev_raw)), ANIMATION_EASING)
#             else:
#                 prev_eased = 1.0

#             current_positions: Dict[int, Tuple[int, int]] = {}
#             current_scales: Dict[int, float] = {}
#             existing_indices = set(animation_state["last_positions"].keys()) | set(animation_state["target_positions"].keys())
#             for idx in existing_indices:
#                 lp = animation_state["last_positions"].get(idx)
#                 tp = animation_state["target_positions"].get(idx)
#                 if lp is not None and tp is not None:
#                     x0, y0 = lp
#                     x1, y1 = tp
#                     cx = int(x0 + (x1 - x0) * prev_eased)
#                     cy = int(y0 + (y1 - y0) * prev_eased)
#                     current_positions[idx] = (cx, cy)
#                 s0 = animation_state["last_scales"].get(idx)
#                 s1 = animation_state["target_scales"].get(idx)
#                 if s0 is not None and s1 is not None:
#                     cs = s0 + (s1 - s0) * prev_eased
#                     current_scales[idx] = cs

#             # Compute new targets using the coalesced visible set
#             new_positions, new_scales = compute_target_layout(planned_visible)
#             animation_state["target_positions"] = new_positions
#             animation_state["target_scales"] = new_scales

#             if animation_state["last_visible_set"] is None:
#                 # First layout: start from targets
#                 animation_state["last_positions"] = new_positions.copy()
#                 animation_state["last_scales"] = new_scales.copy()
#             else:
#                 # Continue from the current in-flight state for smoothness
#                 if current_positions:
#                     animation_state["last_positions"].update(current_positions)
#                 if current_scales:
#                     animation_state["last_scales"].update(current_scales)
#                 # Ensure new indices (e.g., planned but not yet visible) have a baseline
#                 for idx in new_positions.keys():
#                     if idx not in animation_state["last_positions"]:
#                         animation_state["last_positions"][idx] = new_positions[idx]
#                     if idx not in animation_state["last_scales"]:
#                         animation_state["last_scales"][idx] = new_scales[idx]

#             animation_state["animation_start_time"] = global_t
#             animation_state["last_visible_set"] = planned_set

#     def current_interp(global_t: float):
#         ensure_layout_for_time(global_t)
#         in_anim = (
#             animation_state["animation_start_time"] is not None and
#             global_t < animation_state["animation_start_time"] + ANIMATION_DURATION
#         )
#         if not in_anim:
#             animation_state["last_positions"] = animation_state["target_positions"].copy()
#             animation_state["last_scales"] = animation_state["target_scales"].copy()
#             return 1.0

#         raw = (global_t - animation_state["animation_start_time"]) / ANIMATION_DURATION
#         return apply_easing(max(0.0, min(1.0, raw)), ANIMATION_EASING)

#     def make_position_fn(self_idx: int, own_appear_time: float):
#         def pos(local_t: float):
#             local_t = max(0.0, local_t)
#             global_t = own_appear_time + local_t

#             eased = current_interp(global_t)

#             last_pos = animation_state["last_positions"].get(self_idx)
#             target_pos = animation_state["target_positions"].get(self_idx)
#             if last_pos is None or target_pos is None:
#                 return (-99999, -99999)

#             x0, y0 = last_pos
#             x1, y1 = target_pos
#             cx = int(x0 + (x1 - x0) * eased)
#             cy = int(y0 + (y1 - y0) * eased)

#             # Entrance slide-in
#             if local_t < ENTRANCE_DURATION:
#                 p = apply_easing(local_t / ENTRANCE_DURATION, "ease_out")
#                 offset = int(ENTRANCE_TRANSLATE_OFFSET * (1 - p))
#                 cx += offset

#             return (cx, cy)
#         return pos

#     def make_scale_fn(self_idx: int, own_appear_time: float):
#         def scale(local_t: float):
#             local_t = max(0.0, local_t)
#             global_t = own_appear_time + local_t

#             eased = current_interp(global_t)

#             s0 = animation_state["last_scales"].get(self_idx)
#             s1 = animation_state["target_scales"].get(self_idx)
#             if s0 is None or s1 is None:
#                 s = s1 if s1 is not None else 1.0
#             else:
#                 s = s0 + (s1 - s0) * eased

#             # Tiny entrance emphasis
#             if local_t < ENTRANCE_DURATION:
#                 p = apply_easing(local_t / ENTRANCE_DURATION, "ease_out")
#                 s = s * (0.985 + 0.015 * p)

#             return s
#         return scale

#     # Create background
#     background_clip = ImageClip(BACKGROUND_IMAGE)
#     # Scale background to fit video dimensions while maintaining aspect ratio
#     bg_scale_x = VIDEO_WIDTH / background_clip.w
#     bg_scale_y = VIDEO_HEIGHT / background_clip.h
#     bg_scale = max(bg_scale_x, bg_scale_y)  # scale to fill entire frame
#     background_clip = background_clip.resized(bg_scale).with_duration(final_duration)
    
#     # Crop to exact video dimensions if needed
#     if background_clip.w > VIDEO_WIDTH or background_clip.h > VIDEO_HEIGHT:
#         background_clip = background_clip.cropped(
#             x_center=background_clip.w/2, y_center=background_clip.h/2,
#             width=VIDEO_WIDTH, height=VIDEO_HEIGHT
#         )
    
#     # Compose with dynamic position + scale + fade-in
#     composed = [background_clip]
#     for idx, (clip, appear_at, *_rest) in enumerate(base_clips):
#         clip_duration = max(0.01, final_duration - appear_at)
#         fade_dur = min(ENTRANCE_DURATION, clip_duration)

#         animated_clip = (
#             clip
#             .with_start(appear_at)
#             .with_duration(clip_duration)
#             .with_position(make_position_fn(idx, appear_at))
#             .with_effects([
#                 vfx.Resize(make_scale_fn(idx, appear_at)),
#                 vfx.CrossFadeIn(fade_dur),
#             ])
#         )
#         composed.append(animated_clip)

#     CompositeVideoClip(
#         composed,
#         size=(VIDEO_WIDTH, VIDEO_HEIGHT),
#     ).with_duration(final_duration).write_videofile(
#         cache_path,
#         fps=FPS,
#         codec="libx264",
#         audio=False,
#         preset="medium",
#     )

#     return cache_path

from typing import List, Dict, Any, Tuple
from moviepy import ImageClip, CompositeVideoClip, TextClip, VideoClip, vfx
import os
import hashlib
import json
import random

# ===== Constants =====
VIDEO_WIDTH = 1920
VIDEO_HEIGHT = 1080
FPS = 30
BACKGROUND_COLOR = (10, 10, 10)  # RGB

# Layout controls (rows are centered horizontally and vertically)
MAX_ITEMS_PER_ROW = 2            # cap items per row; overflow wraps to next row(s)
H_GAP_MIN = 64                   # minimum horizontal gap inside a row
V_GAP_MIN = 16                   # minimum vertical gap between rows

# Cache settings
CACHE_DIR = "cache/buildshot"
os.makedirs(CACHE_DIR, exist_ok=True)

PADDING_LEFT = 120
PADDING_RIGHT = 120
PADDING_TOP = 120
PADDING_BOTTOM = 120

TEXT_COLOR = (0, 0, 0)
TEXT_FONT_SIZE = 80
FONT_FILE = "font.ttf"
BACKGROUND_IMAGE = "background.png"

POST_HOLD_SECONDS = 3

# Animation settings
ENTRANCE_DURATION = 0.15         # seconds for fade-in and translate-in
ENTRANCE_TRANSLATE_OFFSET = 28   # pixels to slide in from (downwards)
EXIT_DURATION = 0.15             # seconds to fully slide off-screen

# Grouping: items with appearAt within this window belong to the same group
GROUP_WINDOW = 0.60              # seconds

# Max allowed absolute image upscaling relative to its original/native resolution
MAX_IMAGE_UPSCALE_ABS = 1.5
MIN_SCALE = 0.35                 # never scale below this (for readability)

def ease_out_cubic(t: float) -> float:
    t = max(0.0, min(1.0, t))
    return 1 - (1 - t) ** 3

def _cache_path(media_plan: List[Dict[str, Any]], duration: float, font_path: str = "font.ttf", background_path: str = "background.png") -> str:
    media_plan_str = json.dumps(media_plan, sort_keys=True, separators=(',', ':'))
    key_src = f"{duration}|{media_plan_str}|{GROUP_WINDOW}|{EXIT_DURATION}|{MAX_ITEMS_PER_ROW}|{font_path}|{background_path}".encode("utf-8")
    return os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".mp4")

def _hex(rgb: Tuple[int, int, int]) -> str:
    return f"#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}"

def buildShot(media_plan: List[Dict[str, Any]], duration: float, font_path: str = "font.ttf", background_path: str = "background.png") -> str:
    """
    Groups media objects by time windows so items appearing around the same time render together as one group.
    Only one group is on-screen at a time. Groups are laid out as centered rows.
    When the next group starts, the previous group slides fully off-screen in a random (left/top/right) direction.
    """
    if not isinstance(media_plan, list):
        raise ValueError("media_plan must be a list")

    cache_path = _cache_path(media_plan, duration, font_path, background_path)
    if os.path.exists(cache_path):
        return cache_path

    # ---- Build base clips (collect sizes, types, appear times) ----
    items: List[Dict[str, Any]] = []
    for it in media_plan:
        if not (("path" in it and it.get("path")) or ("text" in it and it.get("text") is not None)):
            continue
        appear_at = float(it.get("appearAt", 0) or 0)
        is_image = "path" in it and it.get("path")
        if is_image:
            clip = ImageClip(it["path"])
        else:
            clip = TextClip(
                text=str(it.get("text", "")),
                font_size=TEXT_FONT_SIZE,
                color=_hex(TEXT_COLOR),
                font=font_path,
                size=(VIDEO_WIDTH, VIDEO_HEIGHT),
            )
        items.append({
            "clip": clip,
            "appearAt": appear_at,
            "is_image": is_image,
            "orig_w": clip.w,
            "orig_h": clip.h,
            "idx": len(items),
        })

    if not items:
        from moviepy import ColorClip
        ColorClip(size=(VIDEO_WIDTH, VIDEO_HEIGHT), color=BACKGROUND_COLOR, duration=max(0.1, duration)).write_videofile(
            cache_path, fps=FPS, codec="libx264", audio=False, preset="medium"
        )
        return cache_path

    items.sort(key=lambda x: x["appearAt"])
    last_appear = max(x["appearAt"] for x in items)
    final_duration = float(max(duration, last_appear + POST_HOLD_SECONDS))

    # ---- Group by appearAt window ----
    groups = []
    grp = {"indices": [], "start": None}
    for x in items:
        t = x["appearAt"]
        if not grp["indices"]:
            grp["indices"].append(x["idx"])
            grp["start"] = t
            anchor = t
        else:
            if t - anchor <= GROUP_WINDOW:
                grp["indices"].append(x["idx"])
            else:
                groups.append(grp)
                grp = {"indices": [x["idx"]], "start": t}
                anchor = t
    if grp["indices"]:
        groups.append(grp)

    # compute group end times (end = next group's start; last group holds to final_duration)
    for gi, g in enumerate(groups):
        g["end"] = groups[gi + 1]["start"] if gi + 1 < len(groups) else final_duration

    # deterministic random per media_plan
    seed = int(hashlib.md5(("dirs|" + json.dumps([x["appearAt"] for x in items])).encode()).hexdigest(), 16) & 0xFFFFFFFF
    rng = random.Random(seed)
    for g in groups:
        g["exit_dir"] = rng.choice(["left", "top", "right"])

    # ---- Layout helpers (centered rows) ----
    avail_w = max(1, VIDEO_WIDTH - PADDING_LEFT - PADDING_RIGHT)
    avail_h = max(1, VIDEO_HEIGHT - PADDING_TOP - PADDING_BOTTOM)

    def allowed_scale_limit(idx: int) -> float:
        x = items[idx]
        if not x["is_image"]:
            return float("inf")
        # don't upscale bitmap images too much
        return max(MIN_SCALE, MAX_IMAGE_UPSCALE_ABS)

    def layout_group(g_indices: List[int]) -> Tuple[Dict[int, Tuple[float, float]], Dict[int, float], Dict[int, Tuple[float, float]]]:
        """
        Returns:
          positions[idx] -> (x, y) top-left (for BASE position, no entrance/exit offsets)
          scales[idx]    -> s
          scaled_size[idx] -> (w_s, h_s)
        """
        if not g_indices:
            return {}, {}, {}

        # split into rows
        rows: List[List[int]] = []
        if MAX_ITEMS_PER_ROW <= 0:
            rows.append(g_indices[:])
        else:
            for i in range(0, len(g_indices), MAX_ITEMS_PER_ROW):
                rows.append(g_indices[i:i + MAX_ITEMS_PER_ROW])

        # compute per-item scale: equal width per row; then fit vertically if needed
        scales: Dict[int, float] = {}
        scaled_size: Dict[int, Tuple[float, float]] = {}
        row_heights = []

        # first pass: width-driven scales per row
        for row in rows:
            n = len(row)
            content_w_budget = max(1.0, avail_w - H_GAP_MIN * (n + 1))
            equal_target_w = content_w_budget / n
            # respect per-item upscale caps
            for idx in row:
                base_w = float(items[idx]["orig_w"])
                s = equal_target_w / max(1.0, base_w)
                s = min(s, allowed_scale_limit(idx))
                s = max(MIN_SCALE, s)
                scales[idx] = s
                scaled_size[idx] = (items[idx]["orig_w"] * s, items[idx]["orig_h"] * s)

            row_heights.append(max(scaled_size[idx][1] for idx in row))

        # ensure each row fits horizontally with at least the minimum gaps
        for r, row in enumerate(rows):
            n = len(row)
            max_row_content_w = max(1.0, avail_w - H_GAP_MIN * (n + 1))
            row_content_w = sum(scaled_size[idx][0] for idx in row)
            if row_content_w > max_row_content_w:
                row_shrink = max_row_content_w / max(1.0, row_content_w)
                for idx in row:
                    s = scales[idx] * row_shrink
                    scales[idx] = s
                    scaled_size[idx] = (items[idx]["orig_w"] * s, items[idx]["orig_h"] * s)
        # recompute row heights after horizontal fitting
        row_heights = [max(scaled_size[idx][1] for idx in row) for row in rows]

        # check vertical fit (rows + gaps centered vertically). If too tall, shrink uniformly.
        total_h = sum(row_heights) + V_GAP_MIN * max(0, len(rows) - 1)
        if total_h > avail_h:
            shrink = avail_h / total_h
            for idx in g_indices:
                s = scales[idx] * shrink
                scales[idx] = s
                scaled_size[idx] = (items[idx]["orig_w"] * s, items[idx]["orig_h"] * s)
            # recompute row heights
            row_heights = [max(scaled_size[idx][1] for idx in row) for row in rows]
            total_h = sum(row_heights) + V_GAP_MIN * max(0, len(rows) - 1)

        # horizontal positions per row, then center the whole block vertically
        positions: Dict[int, Tuple[float, float]] = {}
        y_top = PADDING_TOP + (avail_h - total_h) / 2.0
        y_cursor = y_top
        for r, row in enumerate(rows):
            n = len(row)
            row_w_sum = sum(scaled_size[idx][0] for idx in row)
            free_w = max(0.0, avail_w - row_w_sum)
            gap_x = free_w / (n + 1)
            x_cursor = PADDING_LEFT + gap_x
            for idx in row:
                positions[idx] = (x_cursor, y_cursor + (row_heights[r] - scaled_size[idx][1]) / 2.0)
                x_cursor += scaled_size[idx][0] + gap_x
            y_cursor += row_heights[r] + (V_GAP_MIN if r < len(rows) - 1 else 0.0)

        return positions, scales, scaled_size

    # Precompute group layouts
    group_layouts = {}
    for g in groups:
        positions, scales, scaled_size = layout_group(g["indices"])
        group_layouts[id(g)] = (positions, scales, scaled_size)

    # ---- Background ----
    try:
        background_clip = ImageClip(BACKGROUND_IMAGE)
        bg_scale = max(VIDEO_WIDTH / background_clip.w, VIDEO_HEIGHT / background_clip.h)
        background_clip = background_clip.resized(bg_scale).with_duration(final_duration)
        if background_clip.w > VIDEO_WIDTH or background_clip.h > VIDEO_HEIGHT:
            background_clip = background_clip.cropped(
                x_center=background_clip.w / 2, y_center=background_clip.h / 2,
                width=VIDEO_WIDTH, height=VIDEO_HEIGHT
            )
    except Exception:
        # fallback to solid color to avoid crashes if image missing
        from moviepy import ColorClip
        background_clip = ColorClip(size=(VIDEO_WIDTH, VIDEO_HEIGHT), color=BACKGROUND_COLOR, duration=final_duration)

    composed: List[VideoClip] = [background_clip]

    # --- helpers: exact-but-safe offscreen targets + hard integer clamps ---
    def _offscreen_targets(exit_dir: str, bx: int, by: int, w_int: int, h_int: int):
        """
        Use -w+1 / W-1 / -h+1 to avoid 0-width/height slices during mask compose.
        """
        if exit_dir == "left":
            return (-w_int + 1, by)
        elif exit_dir == "right":
            return (VIDEO_WIDTH - 1, by)
        else:  # "top"
            return (bx, -h_int + 1)

    def _clamp_xy(x: int, y: int, w_int: int, h_int: int) -> Tuple[int, int]:
        # Keep top-left within safe compositor bounds (never fully beyond).
        x = max(-w_int + 1, min(VIDEO_WIDTH - 1, x))
        y = max(-h_int + 1, min(VIDEO_HEIGHT - 1, y))
        return x, y

    # ---- Build animated clips per item (entrance + group exit slide) ----
    for g in groups:
        g_id = id(g)
        g_start = g["start"]
        g_end = g["end"]
        exit_dir = g["exit_dir"]
        positions, scales, scaled_size = group_layouts[g_id]

        for idx in g["indices"]:
            base = items[idx]
            clip = base["clip"]
            appear_at = base["appearAt"]

            # duration bounded by the group end (no overlap with next group)
            clip_duration = max(0.01, g_end - appear_at)
            fade_dur = min(ENTRANCE_DURATION, clip_duration)

            # force integer geometry
            base_x = int(round(positions[idx][0]))
            base_y = int(round(positions[idx][1]))
            s = scales[idx]
            w_s, h_s = scaled_size[idx]
            w_int = int(round(w_s))
            h_int = int(round(h_s))
            bx = int(round(base_x))
            by = int(round(base_y))
            ox, oy = _offscreen_targets(exit_dir, bx, by, w_int, h_int)

            def pos_fn_factory(_appear_at, _g_end, bx, by, ox, oy, w_int, h_int):
                def _pos(local_t: float):
                    local_t = 0.0 if local_t is None else max(0.0, local_t)
                    global_t = _appear_at + local_t

                    # entrance: slide up from below
                    if local_t < ENTRANCE_DURATION:
                        p = ease_out_cubic(local_t / ENTRANCE_DURATION)
                        x = bx
                        y = by + int(round(ENTRANCE_TRANSLATE_OFFSET * (1 - p)))
                        return _clamp_xy(int(x), int(y), w_int, h_int)

                    # exit: last EXIT_DURATION of the group
                    if global_t >= (_g_end - EXIT_DURATION):
                        u = (global_t - (_g_end - EXIT_DURATION)) / EXIT_DURATION
                        u = 0.0 if u < 0 else 1.0 if u > 1 else ease_out_cubic(u)
                        x = bx + int(round((ox - bx) * u))
                        y = by + int(round((oy - by) * u))
                        return _clamp_xy(int(x), int(y), w_int, h_int)

                    # hold
                    return _clamp_xy(bx, by, w_int, h_int)
                return _pos

            animated = (
                clip
                .with_start(appear_at)
                .with_duration(clip_duration)
                .with_position(pos_fn_factory(appear_at, g_end, bx, by, ox, oy, w_int, h_int))
                .with_effects([
                    vfx.Resize(s),
                    vfx.CrossFadeIn(fade_dur),
                ])
            )
            composed.append(animated)

    # ---- Compose & render ----
    CompositeVideoClip(composed, size=(VIDEO_WIDTH, VIDEO_HEIGHT)) \
        .with_duration(final_duration) \
        .write_videofile(
            cache_path,
            fps=FPS,
            codec="libx264",
            audio=False,
            preset="medium",
        )

    return cache_path
import os
import subprocess
from typing import List, Tuple, Optional

import numpy as np
from PIL import Image, ImageDraw, ImageFont
from faster_whisper import WhisperModel
from moviepy import VideoFileClip, ImageClip, CompositeVideoClip
from moviepy.video.VideoClip import VideoClip


# ===== Caption Config =====
CAPTION_FONT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "font.ttf")
CAPTION_FONT_SIZE = 72
CAPTION_STROKE_WIDTH = 8
CAPTION_COLOR = "white"
CAPTION_STROKE_COLOR = "black"
CAPTION_HIGHLIGHT_COLOR = "#FFD60A"
CAPTION_HIGHLIGHT_BOLD_OFFSET_PX = 1

CAPTION_Y_RATIO = 0.75  # 3/4 down the screen
CAPTION_MAX_WIDTH_RATIO = 0.90
CAPTION_MAX_LINES = 2
CAPTION_WINDOW_SIZE = 5  # number of words per window; window advances only after last word

# How much context around the current word to show in the window
CAPTION_WORDS_BEFORE = 3
CAPTION_WORDS_AFTER = 3

# Control how long each snippet is visible around the word timing
CAPTION_PRE_ROLL_S = 0.02
CAPTION_POST_ROLL_S = 0.10

# ===== Whisper Config =====
WHISPER_MODEL_SIZE = "small"
WHISPER_DEVICE = "cpu"  # 'cpu' keeps it simple and works everywhere
WHISPER_COMPUTE_TYPE = "int8"


def _transcribe_words(video_path: str) -> List[dict]:
    """
    Transcribe video with faster-whisper and return a flat list of words
    with start/end timestamps: [{"text": str, "start": float, "end": float}].
    """
    model = WhisperModel(WHISPER_MODEL_SIZE, device=WHISPER_DEVICE, compute_type=WHISPER_COMPUTE_TYPE)
    segments, _info = model.transcribe(
        video_path,
        word_timestamps=True,
        vad_filter=True,
        beam_size=5,
        language=None,
    )

    words: List[dict] = []
    for seg in segments:
        if not getattr(seg, "words", None):
            # fallback to segment-level text if words missing
            words.append({
                "text": seg.text.strip(),
                "start": float(seg.start),
                "end": float(seg.end),
            })
            continue
        for w in seg.words:
            text = (w.word or "").strip()
            if not text:
                continue
            words.append({
                "text": text,
                "start": float(w.start),
                "end": float(w.end),
            })
    return words


def _probe_video_nb_frames(path: str) -> int:
    try:
        r1 = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-select_streams",
                "v:0",
                "-show_entries",
                "stream=nb_frames",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                path,
            ],
            capture_output=True,
            text=True,
            check=True,
        )
        txt = r1.stdout.strip()
        if txt and txt != "N/A":
            val = int(txt)
            if val > 0:
                return val
    except Exception:
        pass
    try:
        r2 = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-count_frames",
                "-select_streams",
                "v:0",
                "-show_entries",
                "stream=nb_read_frames",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                path,
            ],
            capture_output=True,
            text=True,
            check=True,
        )
        txt2 = r2.stdout.strip()
        if txt2 and txt2 != "N/A":
            val2 = int(txt2)
            if val2 > 0:
                return val2
    except Exception:
        pass
    return 0


def _probe_video_fps(path: str) -> float:
    try:
        r = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-select_streams",
                "v:0",
                "-show_entries",
                "stream=r_frame_rate",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                path,
            ],
            capture_output=True,
            text=True,
            check=True,
        )
        rate = r.stdout.strip()  # like '60/1'
        if rate and rate != "N/A":
            if "/" in rate:
                num, den = rate.split("/", 1)
                num_f = float(num)
                den_f = float(den)
                if den_f != 0:
                    return num_f / den_f
            else:
                return float(rate)
    except Exception:
        pass
    return 0.0


def _probe_video_stream_duration_seconds(path: str) -> float:
    # Prefer frame count / fps to avoid container-level audio duration
    nb = _probe_video_nb_frames(path)
    fps = _probe_video_fps(path)
    if nb > 0 and fps > 0:
        return nb / fps
    # Fallback to container duration
    try:
        r = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-show_entries",
                "format=duration",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                path,
            ],
            capture_output=True,
            text=True,
            check=True,
        )
        return max(0.0, float(r.stdout.strip()))
    except Exception:
        return 0.0


def _measure_text_bbox(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.FreeTypeFont, stroke_width: int) -> Tuple[int, int]:
    # Returns (width, height)
    if not text:
        return 0, 0
    x0, y0, x1, y1 = draw.textbbox((0, 0), text, font=font, stroke_width=stroke_width)
    return x1 - x0, y1 - y0


def _wrap_words_to_lines(
    words: List[str],
    draw: ImageDraw.ImageDraw,
    font: ImageFont.FreeTypeFont,
    stroke_width: int,
    max_width_px: int,
    max_lines: int,
) -> Tuple[List[List[str]], List[int]]:
    """
    Greedy word-wrap into up to max_lines. Returns (lines, line_widths).
    lines is List[List[str]] where each sublist is words for that line.
    """
    lines: List[List[str]] = [[]]
    line_widths: List[int] = [0]
    for w in words:
        if not lines[-1]:
            candidate = w
        else:
            candidate = " ".join(lines[-1] + [w])
        cand_w, _ = _measure_text_bbox(draw, candidate, font, stroke_width)
        if cand_w <= max_width_px or not lines[-1]:
            # fits or forced into empty line
            lines[-1] = candidate.split(" ")
            line_widths[-1] = cand_w
        else:
            if len(lines) >= max_lines:
                # truncate extra words into last line
                # force append with space even if it exceeds, to avoid dropping text
                lines[-1].append(w)
                line_widths[-1], _ = _measure_text_bbox(draw, " ".join(lines[-1]), font, stroke_width)
            else:
                lines.append([w])
                w_width, _ = _measure_text_bbox(draw, w, font, stroke_width)
                line_widths.append(w_width)
    return lines, line_widths


def _render_caption_image(
    snippet_words: List[str],
    highlight_index: int,
    video_size: Tuple[int, int],
) -> Image.Image:
    """
    Render a transparent image of the caption snippet with the current word highlighted.
    Center horizontally within full video width for easy positioning.
    """
    vw, vh = video_size
    max_text_width = int(vw * CAPTION_MAX_WIDTH_RATIO)

    # Create a temporary small canvas for measurement
    tmp_img = Image.new("RGBA", (max(10, max_text_width), 200), (0, 0, 0, 0))
    draw = ImageDraw.Draw(tmp_img)
    font = ImageFont.truetype(CAPTION_FONT_PATH, CAPTION_FONT_SIZE)

    lines, line_widths = _wrap_words_to_lines(
        snippet_words, draw, font, CAPTION_STROKE_WIDTH, max_text_width, CAPTION_MAX_LINES
    )

    # Compute line height and total height
    ascent, descent = font.getmetrics()
    base_line_height = ascent + descent
    interline_px = int(round(CAPTION_FONT_SIZE * 0.28))
    total_h = base_line_height * len(lines) + interline_px * (len(lines) - 1)

    # Final image spans full video width to simplify centering; height fits text
    img = Image.new("RGBA", (vw, total_h), (0, 0, 0, 0))
    d = ImageDraw.Draw(img)

    # Precompute where the highlight word lands (line index and x offset)
    # Map highlight_index within flattened snippet to line/offset
    flat_words: List[Tuple[int, str]] = []  # (line_idx, word)
    for li, line in enumerate(lines):
        for w in line:
            flat_words.append((li, w))

    if 0 <= highlight_index < len(flat_words):
        hl_line_index, _hl_word = flat_words[highlight_index]
    else:
        hl_line_index = 0

    # Draw lines (base layer: white text with black stroke)
    y_cursor = 0
    per_word_x_offsets: List[List[int]] = []
    for li, line in enumerate(lines):
        text_line = " ".join(line)
        line_w = line_widths[li]
        x_start = (vw - line_w) // 2
        # Track x offsets per word for highlight placement
        x_offsets: List[int] = []
        accum = ""
        for wi, w in enumerate(line):
            before = (accum + (" " if accum else "")) + w
            w_w, _ = _measure_text_bbox(d, before, font, CAPTION_STROKE_WIDTH)
            # offset for current word is width of previous accum
            if accum:
                prev_w, _ = _measure_text_bbox(d, accum, font, CAPTION_STROKE_WIDTH)
            else:
                prev_w = 0
            x_offsets.append(x_start + prev_w)
            accum = before

        d.text(
            (x_start, y_cursor),
            text_line,
            font=font,
            fill=CAPTION_COLOR,
            stroke_width=CAPTION_STROKE_WIDTH,
            stroke_fill=CAPTION_STROKE_COLOR,
            align="center",
        )
        per_word_x_offsets.append(x_offsets)
        y_cursor += base_line_height + interline_px

    # Draw the highlight word on top in its color (simulate bold by overdraw)
    if 0 <= highlight_index < len(flat_words):
        li, _w = flat_words[highlight_index]
        # find word index within that line
        # compute index of highlight within line
        words_before = sum(len(ln) for ln in lines[:li])
        wi_in_line = highlight_index - words_before
        if 0 <= wi_in_line < len(per_word_x_offsets[li]):
            # compute y for that line
            y_for_line = li * (base_line_height + interline_px)
            x_for_word = per_word_x_offsets[li][wi_in_line]
            the_word = lines[li][wi_in_line]
            # Overdraw several times for a bolder look
            offsets = [(0, 0), (CAPTION_HIGHLIGHT_BOLD_OFFSET_PX, 0), (0, CAPTION_HIGHLIGHT_BOLD_OFFSET_PX)]
            for dx, dy in offsets:
                d.text(
                    (x_for_word + dx, y_for_line + dy),
                    the_word,
                    font=font,
                    fill=CAPTION_HIGHLIGHT_COLOR,
                    stroke_width=CAPTION_STROKE_WIDTH,
                    stroke_fill=CAPTION_STROKE_COLOR,
                )

    return img


def add_tiktok_captions(video_path: str, output_path: Optional[str] = None) -> str:
    """
    Transcribe the input video and burn TikTok-style captions.
    Returns the output path written to disk.
    """
    words = _transcribe_words(video_path)
    if not words:
        return video_path

    # Durations
    video_stream_duration = float(_probe_video_stream_duration_seconds(video_path) or 0.0)
    base_raw = VideoFileClip(video_path)
    try:
        audio_duration = float(base_raw.audio.duration) if base_raw.audio else 0.0
    except Exception:
        audio_duration = 0.0
    container_duration = float(base_raw.duration or 0.0)
    target_duration = max(video_stream_duration, audio_duration, container_duration)

    # Freeze last frame after the video stream ends, so video spans full audio
    eps = 1.0 / float(base_raw.fps or 30.0) / 2.0
    safe_last_t = max(0.0, (video_stream_duration or container_duration) - eps)
    last_frame_img = base_raw.get_frame(safe_last_t)

    def _frozen_frame_fn(t: float):
        if t <= (video_stream_duration - eps):
            # Clamp inside available stream range
            tt = max(0.0, min(t, video_stream_duration - eps))
            return base_raw.get_frame(tt)
        return last_frame_img

    base = VideoClip(frame_function=_frozen_frame_fn).with_duration(target_duration)
    base.fps = base_raw.fps
    base.size = base_raw.size
    if base_raw.audio is not None:
        base.audio = base_raw.audio
    vw, vh = base.w, base.h
    clips: List[ImageClip] = []

    total_dur = float(target_duration)
    window_size = max(1, int(CAPTION_WINDOW_SIZE))
    prev_end = 0.0

    # --- Split into sentences by terminal punctuation on words ---
    sentence_boundaries: List[Tuple[int, int]] = []  # (start_idx, end_idx_exclusive)
    s_start = 0
    n = len(words)
    terminal_chars = set([".", "!", "?", "…"])
    for i, wobj in enumerate(words):
        wtxt = str(wobj.get("text", ""))
        ends_sentence = any(ch in wtxt for ch in terminal_chars)
        is_last = (i == n - 1)
        if ends_sentence or is_last:
            end_idx = i + 1
            if end_idx > s_start:
                sentence_boundaries.append((s_start, end_idx))
            s_start = end_idx
    if s_start < n:
        sentence_boundaries.append((s_start, n))

    # --- Build clips: window within each sentence, highlight word-by-word ---
    for (s_start, s_end) in sentence_boundaries:
        idx = s_start
        while idx < s_end:
            win_start = idx
            win_end = min(s_end, win_start + window_size)
            snippet = [words[i]["text"] for i in range(win_start, win_end)]

            for wi in range(win_start, win_end):
                local_index = wi - win_start
                w = words[wi]

                if wi == win_start:
                    desired_start = max(0.0, float(w["start"]) - CAPTION_PRE_ROLL_S)
                    start_time = max(prev_end, desired_start)
                else:
                    start_time = prev_end

                if wi < win_end - 1:
                    next_start = float(words[wi + 1]["start"])  # until next word begins
                else:
                    # last word in window: hold until next window OR next sentence OR end
                    if win_end < s_end:
                        next_start = float(words[win_end]["start"])  # next window in same sentence
                    elif (sentence_boundaries and (sentence_boundaries[-1] != (s_start, s_end))):
                        # next sentence exists
                        cur_idx = sentence_boundaries.index((s_start, s_end))
                        if cur_idx + 1 < len(sentence_boundaries):
                            next_sentence_first = sentence_boundaries[cur_idx + 1][0]
                            next_start = float(words[next_sentence_first]["start"]) if next_sentence_first < n else total_dur
                        else:
                            next_start = total_dur
                    else:
                        next_start = total_dur
                    next_start = min(next_start, float(w["end"]) + CAPTION_POST_ROLL_S, total_dur)

                end_time = min(total_dur, next_start)
                if end_time < start_time:
                    end_time = start_time

                duration = end_time - start_time
                if duration <= 0:
                    continue

                img = _render_caption_image(snippet, local_index, (vw, vh))
                np_img = np.array(img)
                cap_clip = ImageClip(np_img).with_start(start_time).with_duration(duration)

                y_center = int(vh * CAPTION_Y_RATIO)
                y_pos = int(y_center - (cap_clip.h // 2))
                cap_clip = cap_clip.with_position(("center", y_pos))
                clips.append(cap_clip)

                prev_end = end_time

            idx = win_end

    # Ensure composite has a defined duration matching the target timeline (audio end)
    comp = CompositeVideoClip([base] + clips, size=(vw, vh)).with_duration(total_dur)

    if output_path is None:
        base_dir, base_name = os.path.split(video_path)
        name, ext = os.path.splitext(base_name)
        output_path = os.path.join(base_dir or ".", f"{name}_captions{ext}")

    # Avoid writing to the same file being read
    in_abs = os.path.abspath(video_path)
    out_abs = os.path.abspath(output_path)
    same_path = in_abs == out_abs
    if same_path:
        base_dir, base_name = os.path.split(in_abs)
        name, ext = os.path.splitext(base_name)
        target_path = os.path.join(base_dir or ".", f"{name}_captions{ext}")
    else:
        target_path = out_abs

    comp.write_videofile(
        target_path,
        codec="libx264",
        audio_codec="aac",
        fps=base.fps or 30,
        preset="medium",
        threads=0,
        logger=None,
    )

    base.close()
    base_raw.close()
    comp.close()
    for c in clips:
        c.close()

    return target_path


__all__ = ["add_tiktok_captions"]

import subprocess
import os
import json
from typing import List


def _has_audio_stream(path: str) -> bool:
    try:
        result = subprocess.run([
            'ffprobe', '-v', 'error', '-print_format', 'json', '-show_streams', path
        ], capture_output=True, text=True, check=True)
        info = json.loads(result.stdout)
        for s in info.get('streams', []):
            if s.get('codec_type') == 'audio':
                return True
        return False
    except Exception:
        # If probe fails, assume it has audio to avoid attempting to synthesize incorrectly
        return True


def combineVideos(video_paths: List[str], output_path: str) -> str:
    """Combine multiple videos into a single video using ffmpeg.
    
    Args:
        video_paths: List of paths to input video files
        output_path: Path where the combined video will be saved
        
    Returns:
        Path to the combined video file
    """
    if not video_paths:
        raise ValueError("video_paths cannot be empty")
    
    # Verify all input files exist
    for path in video_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"Video file not found: {path}")
    
    # Normalize paths for cross-platform compatibility
    video_paths = [os.path.normpath(path) for path in video_paths]
    output_path = os.path.normpath(output_path)
    
    # Normalize all inputs to consistent params (video: 1920x1080 30fps yuv420p h264; audio: AAC LC 48kHz stereo)
    # Use absolute path for normalization directory to avoid relative path duplication in concat list
    norm_dir = os.path.abspath(os.path.join(os.path.dirname(output_path) or '.', '__concat_norm__'))
    os.makedirs(norm_dir, exist_ok=True)
    normalized_paths: List[str] = []

    for i, in_path in enumerate(video_paths):
        norm_path = os.path.join(norm_dir, f"{i:04d}.mp4")
        has_audio = _has_audio_stream(in_path)

        if has_audio:
            cmd = [
                'ffmpeg', '-hide_banner', '-loglevel', 'error',
                '-i', in_path,
                '-c:v', 'libx264',
                '-pix_fmt', 'yuv420p',
                '-r', '30',
                '-vsync', 'cfr',
                '-c:a', 'aac',
                '-b:a', '192k',
                '-ar', '48000',               # resample to 48k for concat stability
                '-ac', '2',
                '-af', 'aresample=async=1:first_pts=0',
                '-movflags', '+faststart',
                '-y', norm_path
            ]
        else:
            # Add a silent stereo track if missing
            cmd = [
                'ffmpeg', '-hide_banner', '-loglevel', 'error',
                '-i', in_path,
                '-f', 'lavfi', '-t', '1', '-i', 'anullsrc=r=48000:cl=stereo',
                '-shortest',
                '-c:v', 'libx264',
                '-pix_fmt', 'yuv420p',
                '-r', '30',
                '-vsync', 'cfr',
                '-c:a', 'aac',
                '-b:a', '192k',
                '-ar', '48000',
                '-ac', '2',
                '-movflags', '+faststart',
                '-map', '0:v:0',
                '-map', '1:a:0',
                '-y', norm_path
            ]

        print(f"Running ffmpeg command: {' '.join(cmd)}")
        subprocess.run(cmd, check=True, capture_output=True, text=True)
        normalized_paths.append(norm_path)

    # Create temporary concat file for ffmpeg (absolute path ensures ffmpeg resolves correctly)
    concat_file = os.path.abspath(output_path).replace('.mp4', '_concat.txt')
    
    try:
        # Write concat file with absolute, forward-slashed paths per ffmpeg concat demuxer best practices
        with open(concat_file, 'w') as f:
            for path in normalized_paths:
                abs_path = os.path.abspath(path)
                # ffmpeg on Windows is happier with forward slashes
                abs_path = abs_path.replace('\\', '/')
                f.write(f"file '{abs_path}'\n")
        
        # Concatenate normalized videos; re-encode audio to avoid timestamp gaps
        cmd = [
            'ffmpeg',
            '-hide_banner', '-loglevel', 'error',
            '-f', 'concat',
            '-safe', '0',
            '-i', concat_file,
            '-c:v', 'copy',
            '-c:a', 'aac',
            '-b:a', '192k',
            '-ar', '48000',
            '-ac', '2',
            '-af', 'aresample=async=1:first_pts=0',
            '-movflags', '+faststart',
            '-y',
            output_path
        ]
        
        print(f"Running ffmpeg command: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        
        # Print ffmpeg output for debugging
        if result.stdout:
            print(f"FFmpeg stdout: {result.stdout}")
        if result.stderr:
            print(f"FFmpeg stderr: {result.stderr}")
        
        return output_path
        
    except subprocess.CalledProcessError as e:
        print(f"FFmpeg command failed with exit code: {e.returncode}")
        if e.stdout:
            print(f"FFmpeg stdout: {e.stdout}")
        if e.stderr:
            print(f"FFmpeg stderr: {e.stderr}")
        
        raise RuntimeError(f"Failed to combine videos. Exit code: {e.returncode}, stderr: {e.stderr}")
    except Exception as e:
        raise
    finally:
        # Clean up temporary concat file
        if os.path.exists(concat_file):
            os.remove(concat_file)
        # Clean up normalized files
        try:
            for p in normalized_paths:
                if os.path.exists(p):
                    os.remove(p)
            if os.path.isdir(norm_dir) and not os.listdir(norm_dir):
                os.rmdir(norm_dir)
        except Exception:
            pass
import subprocess
import os

def create9x16Video(input_video_path: str, output_path: str) -> str:
    """
    Convert a video to 9:16 aspect ratio with the original video centered
    and a blurred version as the background.
    
    Args:
        input_video_path: Path to the input video file
        output_path: Path where the 9:16 video will be saved
        
    Returns:
        Path to the output 9:16 video file
    """
    if not os.path.exists(input_video_path):
        raise FileNotFoundError(f"Input video file not found: {input_video_path}")
    
    # Normalize paths
    input_video_path = os.path.normpath(input_video_path)
    output_path = os.path.normpath(output_path)
    
    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # ffmpeg command to create 9:16 video with blurred background
    cmd = [
        'ffmpeg',
        '-hide_banner', '-loglevel', 'error',
        '-i', input_video_path,  # Input video
        '-filter_complex', 
        # Create blurred background that fills 9:16 (1080x1920)
        '[0:v]scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920,gblur=sigma=50[bg];'
        # Scale original video to fit width while maintaining aspect ratio
        '[0:v]scale=1080:-1[fg];'
        # Overlay the original video centered on the blurred background
        '[bg][fg]overlay=(W-w)/2:(H-h)/2[v]',
        '-map', '[v]',
        '-map', '0:a?',  # Map audio if it exists
        '-c:v', 'libx264',
        '-pix_fmt', 'yuv420p',
        '-r', '30',
        '-c:a', 'aac',
        '-b:a', '192k',
        '-ar', '48000',
        '-ac', '2',
        '-movflags', '+faststart',
        '-y',
        output_path
    ]
    
    print(f"Creating 9:16 video: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        
        # Print ffmpeg output for debugging
        if result.stdout:
            print(f"FFmpeg stdout: {result.stdout}")
        if result.stderr:
            print(f"FFmpeg stderr: {result.stderr}")
        
        return output_path
        
    except subprocess.CalledProcessError as e:
        print(f"FFmpeg command failed with exit code: {e.returncode}")
        if e.stdout:
            print(f"FFmpeg stdout: {e.stdout}")
        if e.stderr:
            print(f"FFmpeg stderr: {e.stderr}")
        
        raise RuntimeError(f"Failed to create 9:16 video. Exit code: {e.returncode}, stderr: {e.stderr}")import os
import time
import io
from typing import Optional, Union
import requests
from google import genai
from google.genai import types
from dotenv import load_dotenv
import hashlib
import json
import imghdr

# Load environment variables from .env file
load_dotenv()

# ---------------------------------------------------------------------------
# Simple on-disk cache for Gemini responses
# ---------------------------------------------------------------------------
CACHE_DIR = os.path.join(os.path.dirname(__file__), "geminicache")
os.makedirs(CACHE_DIR, exist_ok=True)


def _cache_key(params: dict) -> str:
    """Return a stable SHA256 hash for *params* dict.*"""
    canonical = json.dumps(params, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
    return hashlib.sha256(canonical.encode("utf-8")).hexdigest()


def _load_cache(key: str) -> Optional[str]:
    """Load cached response text if available, otherwise *None*."""
    path = os.path.join(CACHE_DIR, f"{key}.json")
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("response")
    except Exception as e:
        print(f"Warning: failed to read cache {path}: {e}")
        return None


def _save_cache(key: str, params: dict, response: str) -> None:
    """Persist *response* (and *params* for debugging) to the cache."""
    path = os.path.join(CACHE_DIR, f"{key}.json")
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump({"params": params, "response": response, "timestamp": time.time()}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Warning: failed to write cache {path}: {e}")

# ---------------------------------------------------------------------------
# Re-use a single Client instance so we do not create new gRPC pools for every
# clip.  This keeps memory stable across long sessions.
# ---------------------------------------------------------------------------

_CLIENT: Optional[genai.Client] = None


def _get_client(api_key: str) -> genai.Client:
    global _CLIENT
    if _CLIENT is None:
        _CLIENT = genai.Client(api_key=api_key)
    return _CLIENT

#NOTE THAT WE SHOULD ALWAYS BE USING GEMINI-2.5-FLASH, THIS IS NOT A TYPO.

def ask_gemini(prompt: str, api_key: Optional[str] = None, model: str = "gemini-2.5-flash", max_retries: int = 3) -> str:
    """
    Ask Gemini a text-only question using the official Google Gemini package.
    
    Args:
        prompt: The text prompt to send to Gemini
        api_key: Optional API key (defaults to GEMINI_API_KEY environment variable)
        model: The Gemini model to use (default: gemini-2.5-flash)
        max_retries: Maximum number of retries if response is empty (default: 3)
        
    Returns:
        Gemini's response as a string
    """
    api_key = api_key or os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("GEMINI_API_KEY environment variable not set. Please create a .env file with GEMINI_API_KEY=your_api_key_here")

    # ---- Cache lookup ----
    _cache_params = {"prompt": prompt, "model": model}
    _cache_key_val = _cache_key(_cache_params)
    _cached = _load_cache(_cache_key_val)
    if _cached is not None:
        return _cached
    
    client = _get_client(api_key)
    
    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(
                model=model,
                contents=prompt
            )
            
            # Check if response is empty after trimming
            response_text = response.text.strip() if response.text else ""
            if response_text:
                _save_cache(_cache_key_val, _cache_params, response_text)
                return response_text
            else:
                print(f"Attempt {attempt + 1}/{max_retries}: Received empty response, retrying...")
                if attempt < max_retries - 1:
                    time.sleep(2)  # Wait 2 seconds before retrying
                    
        except Exception as e:
            if attempt == max_retries - 1:  # Last attempt
                raise Exception(f"Gemini API request failed: {str(e)}")
            print(f"Attempt {attempt + 1}/{max_retries} failed: {str(e)}, retrying...")
            time.sleep(2)
    
    # If we get here, all attempts returned empty responses
    print(f"Warning: All {max_retries} attempts returned empty responses")
    return ""


def wait_for_file_activation(client, file_name: str, max_wait_time: int = 300) -> bool:
    """
    Wait for a file to become ACTIVE before using it.
    
    Args:
        client: Gemini client instance
        file_name: Name of the uploaded file
        max_wait_time: Maximum time to wait in seconds (default: 5 minutes)
        
    Returns:
        True if file becomes active, False if timeout or failed
    """
    start_time = time.time()
    while time.time() - start_time < max_wait_time:
        try:
            file_info = client.files.get(name=file_name)
            if file_info.state == "ACTIVE":
                return True
            elif file_info.state == "FAILED":
                print(f"File {file_name} failed to activate, deleting it...")
                try:
                    client.files.delete(name=file_name)
                    print(f"Successfully deleted failed file: {file_name}")
                except Exception as delete_error:
                    print(f"Warning: Failed to delete failed file {file_name}: {delete_error}")
                return False
            print(f"Waiting for file {file_name} to activate... Current state: {file_info.state}")
            time.sleep(5)  # Wait 5 seconds before checking again
        except Exception as e:
            print(f"Error checking file state: {e}")
            time.sleep(5)
    
    return False


def ask_gemini_with_video(video_path: str, prompt: str, api_key: Optional[str] = None, max_upload_retries: int = 3, max_content_retries: int = 3, model: str = "gemini-2.5-flash") -> str:
    api_key = api_key or os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("GEMINI_API_KEY environment variable not set. Please create a .env file with GEMINI_API_KEY=your_api_key_here")

    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video file not found: {video_path}")

    # ---- Cache lookup ----
    _vid_cache_params = {"prompt": prompt, "model": model, "video_path": video_path}
    _vid_cache_key = _cache_key(_vid_cache_params)
    _vid_cached = _load_cache(_vid_cache_key)
    if _vid_cached is not None:
        return _vid_cached

    client = _get_client(api_key)
    uploaded_file = None
    
    def generate_content_with_retry(response_func):
        """Helper function to retry content generation if response is empty"""
        for content_attempt in range(max_content_retries):
            try:
                response = response_func()
                response_text = response.text.strip() if response.text else ""
                if response_text:
                    return response_text
                else:
                    print(f"Content attempt {content_attempt + 1}/{max_content_retries}: Received empty response, retrying...")
                    if content_attempt < max_content_retries - 1:
                        time.sleep(2)
            except Exception as e:
                if content_attempt == max_content_retries - 1:
                    raise e
                print(f"Content attempt {content_attempt + 1}/{max_content_retries} failed: {str(e)}, retrying...")
                time.sleep(2)
        
        print(f"Warning: All {max_content_retries} content generation attempts returned empty responses")
        return ""
    
    try:
        # Check file size to determine upload method
        file_size = os.path.getsize(video_path)
        print(f"File size: {file_size / (1024*1024):.2f} MB")
        
        # Use resumable upload with manual chunked streaming
        print("Using resumable file upload (chunked streaming)…")

        for attempt in range(max_upload_retries):
            try:
                print(f"Upload attempt {attempt + 1}/{max_upload_retries}")

                file_name = upload_file_resumable(video_path, api_key)

                # Wait for file to become ACTIVE before we can use it
                print("Waiting for file to activate…")
                if not wait_for_file_activation(client, file_name):
                    raise RuntimeError("File failed to activate after upload")

                uploaded_file = client.files.get(name=file_name)

                def file_api_response():
                    return client.models.generate_content(
                        model=model,
                        contents=[uploaded_file, prompt]
                    )

                _vid_result = generate_content_with_retry(file_api_response)
                _save_cache(_vid_cache_key, _vid_cache_params, _vid_result)
                return _vid_result

            except Exception as upload_error:
                print(f"Upload attempt {attempt + 1} failed: {upload_error}")

                # Best-effort cleanup (no need to fail if already deleted)
                try:
                    if uploaded_file:
                        client.files.delete(name=uploaded_file.name)
                except Exception:
                    pass

                uploaded_file = None

                if attempt < max_upload_retries - 1:
                    print("Retrying upload in 5 seconds…")
                    time.sleep(5)
                else:
                    raise Exception(f"Failed to upload and activate file after {max_upload_retries} attempts: {upload_error}")
        
        # This should never be reached, but just in case
        return ""
        
    except Exception as e:
        raise Exception(f"Failed to analyze video {video_path}: {str(e)}")
    finally:
        # Clean up uploaded file if it exists
        if uploaded_file:
            try:
                print(f"Cleaning up file: {uploaded_file.name}")
                client.files.delete(name=uploaded_file.name)
                print("File cleanup successful")
            except Exception as cleanup_error:
                print(f"Warning: Failed to cleanup file {uploaded_file.name}: {cleanup_error}")


def ask_gemini_with_images(image_paths: list[str], prompt: str, api_key: Optional[str] = None, model: str = "gemini-2.5-flash", max_retries: int = 3) -> str:
    """
    Ask Gemini by supplying multiple local image files alongside a text *prompt*.

    Args:
        image_paths: List of local file paths to images that will be sent to Gemini.
        prompt: The text prompt to accompany the images.
        api_key: Optional Gemini API key (falls back to GEMINI_API_KEY env var).
        model: Gemini model to use (default: gemini-2.5-flash).
        max_retries: Maximum number of retries if Gemini returns an empty response.

    Returns:
        The textual response from Gemini.
    """
    api_key = api_key or os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise ValueError("GEMINI_API_KEY environment variable not set. Please create a .env file with GEMINI_API_KEY=your_api_key_here")

    # ---- Cache lookup ----
    _img_paths_sorted = sorted([str(p) for p in image_paths])
    _img_cache_params = {"prompt": prompt, "model": model, "image_paths": _img_paths_sorted}
    _img_cache_key = _cache_key(_img_cache_params)
    _img_cached = _load_cache(_img_cache_key)
    if _img_cached is not None:
        return _img_cached

    client = _get_client(api_key)

    # Helper: detect correct MIME type from actual file contents
    def _detect_mime_type(path: str) -> Optional[str]:
        kind = imghdr.what(path)
        if not kind:
            return None
        mapping = {
            "jpeg": "image/jpeg",
            "png": "image/png",
            "gif": "image/gif",
            "bmp": "image/bmp",
            "tiff": "image/tiff",
            "webp": "image/webp",
        }
        return mapping.get(kind)

    # Build the list of Part objects: one image Part per image, then the prompt Part
    image_parts: list[types.PartUnion] = []
    safe_image_paths: list[str] = []
    for path in image_paths:
        if not os.path.exists(path):
            print(f"Warning: image not found: {path}. Skipping.")
            continue

        mime_type = _detect_mime_type(path)
        if not mime_type:
            print(f"Warning: could not determine MIME type for {path}. Skipping.")
            continue
        try:
            with open(path, "rb") as f:
                data_bytes = f.read()
            image_parts.append(types.Part.from_bytes(data=data_bytes, mime_type=mime_type))
            safe_image_paths.append(path)
        except Exception as e:
            print(f"Failed to load image {path}: {e}")

    # Always include the text prompt
    def _build_contents(parts: list[types.PartUnion]) -> list[types.PartUnion]:
        return parts + [types.Part.from_text(text=prompt)]

    # Try full set; on repeated failures, progressively fall back to JPEG/PNG only, then text-only
    attempts: list[list[types.PartUnion]] = []
    attempts.append(_build_contents(image_parts))
    if image_parts:
        # Filter to just JPEG/PNG as a safer subset
        jpeg_png_parts: list[types.PartUnion] = []
        for pth, part in zip(safe_image_paths, image_parts):
            mime = _detect_mime_type(pth)
            if mime in {"image/jpeg", "image/png"}:
                jpeg_png_parts.append(part)
        if jpeg_png_parts and len(jpeg_png_parts) != len(image_parts):
            attempts.append(_build_contents(jpeg_png_parts))
    # Final fallback: text-only
    attempts.append(_build_contents([]))

    last_error: Optional[Exception] = None
    for attempt_contents in attempts:
        for attempt in range(max_retries):
            try:
                response = client.models.generate_content(
                    model=model,
                    contents=attempt_contents,
                )
                response_text = response.text.strip() if response.text else ""
                if response_text:
                    _save_cache(_img_cache_key, _img_cache_params, response_text)
                    return response_text
                else:
                    print(f"Attempt {attempt + 1}/{max_retries}: Received empty response, retrying…")
            except Exception as e:
                last_error = e
                print(f"Attempt {attempt + 1}/{max_retries} failed: {str(e)}, retrying…")
            time.sleep(2)

    # If we reach here, all attempts failed or returned empty. Return a safe fallback JSON the caller can parse.
    print("Warning: Gemini image analysis failed after all fallbacks. Returning safe default selection.")
    if last_error:
        print(f"Last error: {last_error}")
    fallback = {
        "analysis": "Fallback selected due to image processing issues. Defaulting to first image.",
        "finalSelection": 1,
    }
    fallback_str = json.dumps(fallback)
    _save_cache(_img_cache_key, _img_cache_params, fallback_str)
    return fallback_str


def upload_file_resumable(file_path: str, api_key: str, chunk_size: int = 5*8 * 1024 * 1024) -> str:
    """Upload *file_path* to Gemini using a resumable, chunked upload.

    This avoids loading the full video into RAM by streaming the file in
    ``chunk_size`` byte pieces. Returns the server-generated file *name*
    (e.g. ``files/abcd1234``) which can be passed to other File API calls.
    """

    SESSION_URL = "https://generativelanguage.googleapis.com/upload/v1beta/files"

    total_size = os.path.getsize(file_path)

    print(f"[UPLOAD] Starting resumable upload → {os.path.basename(file_path)}  "
          f"{total_size / (1024 * 1024):.2f} MB in chunks of {chunk_size / (1024 * 1024):.2f} MB")

    # 1. Initiate a resumable upload session
    init_headers = {
        "X-Goog-Upload-Protocol": "resumable",
        "X-Goog-Upload-Command": "start",
        "X-Goog-Upload-Header-Content-Type": "video/mp4",
        "X-Goog-Upload-Header-Content-Length": str(total_size),
        "Content-Type": "application/json",
    }

    init_payload = {
        "file": {
            "display_name": os.path.basename(file_path)
        }
    }

    init_resp = requests.post(
        f"{SESSION_URL}?uploadType=resumable&key={api_key}",
        headers=init_headers,
        json=init_payload,
        timeout=120,
    )

    if init_resp.status_code not in {200, 201}:
        raise RuntimeError(f"Could not initiate upload session: {init_resp.text}")

    upload_url = init_resp.headers.get("X-Goog-Upload-URL") or init_resp.headers.get("x-goog-upload-url")
    if not upload_url:
        raise RuntimeError("Upload URL not returned by Gemini API")

    print(f"[UPLOAD] Resumable session URL received")

    offset = 0

    with open(file_path, "rb") as f:
        while True:
            buf = f.read(chunk_size)
            if not buf:
                break

            headers = {
                "Content-Type": "video/mp4",
                "Content-Length": str(len(buf)),
                "X-Goog-Upload-Offset": str(offset),
                "X-Goog-Upload-Command": "upload, finalize" if offset + len(buf) == total_size else "upload",
                "X-Goog-Upload-Protocol": "resumable",
            }

            put_resp = requests.put(upload_url, headers=headers, data=io.BytesIO(buf), timeout=300)

            if put_resp.status_code not in {200, 201, 308}:
                raise RuntimeError(
                    f"Chunk upload failed at offset {offset}: {put_resp.status_code} – {put_resp.text}"
                )

            offset += len(buf)

            status_type = "FINAL" if headers["X-Goog-Upload-Command"].startswith("upload, finalize") else "CHUNK"
            print(f"[UPLOAD] {status_type} OK – bytes {offset-len(buf)}-{offset-1}  "
                  f"({offset / total_size * 100:.1f}% done)  → HTTP {put_resp.status_code}")

    print("[UPLOAD] All chunks uploaded – waiting for server to finalise file resource …")

    # --- Parse the final response ----------------------------
    # The Gemini Files API may respond with either of these shapes:
    # 1. {"file": {"name": "files/abc-123", ...}}
    # 2. {"name": "files/abc-123", ...}
    # Accept both to stay forward-compatible.

    try:
        file_info_raw = put_resp.json()
    except ValueError:
        # Service returned non-JSON – surface the raw text to help debugging.
        raise RuntimeError(
            f"Upload finished but response was not valid JSON: {put_resp.text}"
        )

    file_resource = file_info_raw["file"] if "file" in file_info_raw else file_info_raw

    name = file_resource.get("name")
    if not name:
        raise RuntimeError(
            f"Upload finished but no file name in response: {file_info_raw}"
        )

    print(f"[UPLOAD] Upload complete – file name: {name}")
    return name
import subprocess
import json

def getAudioLength(audio_path: str) -> float:
    """Get the length of an audio file in seconds using ffmpeg.
    
    Args:
        audio_path: Path to the audio file
        
    Returns:
        Duration in seconds as a float
    """
    try:
        result = subprocess.run([
            'ffprobe', 
            '-v', 'quiet',
            '-print_format', 'json',
            '-show_format',
            audio_path
        ], capture_output=True, text=True, check=True)
        
        data = json.loads(result.stdout)
        duration = float(data['format']['duration'])
        return duration
    except Exception as e:
        raise RuntimeError(f"Failed to get audio length for {audio_path}: {e}")
from dotenv import load_dotenv
import os
import requests
import json
from urllib.parse import quote_plus
from pathlib import Path

load_dotenv()

gemini_prompt="""What number image is best for the following criteria
{description}

1. Shouldn't be edited, like it ideally shouldnt be a collage or have text overlay
2. Should be high quality 
3. MUST NOT HAVE ANY BRANDING, LIKE A NEWS COMPANY OR A STOCK IMAGE COMPANY OR ANYTHING SIMILAR UNLESS IT IS A PART OF THE IMAGE OBJECT ITSELF.
    - ABSOLUTELY NO STOCK IMAGE WATERMARKS LIKE "ALAMY", "GETTY", "PEXELS", "STOCK", etc

Output your response in this json:
{{
"analysis": string,  # reasoning here, look at all the images, compare, etc
"finalSelection": int
}}

Your analysis value must be a simple string that must follow this pattern:
For each image, do the following:
1. See if it follows the theme
2. See if it follows EACH individual criteria
Then, only after looking at and rating all the images, do the following:
Explain which one is the best fit for the criteria and why.

Remember, JUST A JSON BLOCK WITH THE JSON OBJECT, NO OTHER TEXT.
"""

# ------------------ Internal helpers ------------------


def _download_images(search_query: str, num_images: int = 10) -> list[str]:
    """Search Serper for *search_query*, download up to *num_images* images, and
    return a list of local file paths."""

    safe_query = quote_plus(search_query)
    folder = Path("images") / safe_query
    
    # Check if folder already exists and has images
    if folder.exists():
        existing_images = list(folder.glob("*"))
        if existing_images:
            return [str(path) for path in existing_images]
    
    api_key = os.getenv("SERPER_API_KEY")
    if not api_key:
        raise ValueError("SERPER_API_KEY environment variable not set.")

    headers = {
        "X-API-KEY": api_key,
        "Content-Type": "application/json",
    }

    search_query = f"{search_query} -filetype:gif"
    response = requests.post(
        "https://google.serper.dev/images",
        headers=headers,
        json={"q": search_query,"num": num_images, 
        # "tbs": "isz:lt,islt:xga",
        },
        timeout=30,
    )
    response.raise_for_status()
    data = response.json()

    results = data.get("images") or data.get("results") or data.get("items") or []

    folder.mkdir(parents=True, exist_ok=True)

    image_paths: list[str] = []

    for idx, item in enumerate(results[:num_images], start=1):
        url = (
            item.get("imageUrl")
            or item.get("url")
            or item.get("thumbnailUrl")
        )
        if not url:
            continue

        try:
            img_resp = requests.get(url, timeout=20)
            if img_resp.status_code != 200:
                continue

            ext = ".png" if url.lower().endswith(".png") else ".jpg"
            file_path = folder / f"{idx}{ext}"
            with open(file_path, "wb") as f:
                f.write(img_resp.content)
            image_paths.append(str(file_path))
        except Exception:
            continue

    if not image_paths:
        raise RuntimeError("Failed to download any images.")

    return image_paths

# ------------------ Public API ------------------


def getImage(searchQuery: str, description: str) -> str:
    """Search for images of *searchQuery*, ask Gemini which one is best given
    *description*, and return the selected image's local path."""

    from gemini import ask_gemini_with_images  # Local import to avoid circular deps

    # --- Helper to validate images ---
    import imghdr

    def _filter_valid_images(paths: list[str]) -> list[str]:
        """Return only those image *paths* that are valid and ensure the
        folder contains a contiguous sequence of image filenames (1.jpg,
        2.jpg, 3.png, ...).

        Validation strategy:
        1. Skip zero-byte files.
        2. Use the stdlib *imghdr* module to identify common image formats.
           If *imghdr.what* returns *None* the file is considered invalid or
           corrupted and will be deleted from disk.
        After validation, any gaps in numbering caused by deletions are
        fixed by renaming the remaining images in ascending order so that
        the filenames are sequential with no jumps.
        """
        valid: list[str] = []
        for p in paths:
            try:
                remove_file = False
                if os.path.getsize(p) == 0:
                    remove_file = True
                elif imghdr.what(p) is None:
                    remove_file = True

                if remove_file:
                    # Delete corrupted/invalid file so it doesn't linger on disk
                    try:
                        os.remove(p)
                    except OSError:
                        pass  # Ignore inability to delete
                    continue  # Skip to next path

                valid.append(p)
            except Exception:
                # Any unexpected error counts as invalid – attempt deletion
                try:
                    os.remove(p)
                except OSError:
                    pass
                continue

        # ---------- Renumber remaining valid images ----------
        from pathlib import Path as _Path

        # Sort paths by the numeric value in their stem ("3" in "3.jpg").
        def _numeric_stem(path: str) -> int:
            try:
                return int(_Path(path).stem)
            except ValueError:
                # Non-numeric stems are sorted to the end
                return 10**9

        valid_sorted = sorted(valid, key=_numeric_stem)
        renumbered_paths: list[str] = []

        for new_idx, old_path in enumerate(valid_sorted, start=1):
            old_p = _Path(old_path)
            new_name = f"{new_idx}{old_p.suffix}"
            new_p = old_p.with_name(new_name)

            # If the current name already matches the desired one, skip renaming
            if old_p == new_p:
                renumbered_paths.append(str(old_p))
                continue

            # Handle potential name collision by first renaming the target (rare)
            if new_p.exists() and new_p != old_p:
                temp_p = new_p.with_name(f"tmp_{new_idx}{old_p.suffix}")
                new_p.rename(temp_p)

            old_p.rename(new_p)
            renumbered_paths.append(str(new_p))

        return renumbered_paths

    # 1. Fetch and store images
    image_paths = _download_images(searchQuery)

    # 1b. Filter out any corrupted / invalid images before sending to Gemini
    image_paths = _filter_valid_images(image_paths)

    if not image_paths:
        raise RuntimeError("No valid images were downloaded for the given search query.")

    # 2. Build the prompt
    prompt = gemini_prompt.format(description=description)

    # 3. Ask Gemini for the best image
    response = ask_gemini_with_images(image_paths, prompt)

    # 4. Parse Gemini's response to determine the chosen index
    import re

    def _extract_final_selection(resp: str) -> int | None:
        """Try to robustly pull the integer value of `finalSelection` from *resp*.

        The Gemini response is often wrapped in a markdown fenced code block like:

        ```json
        { "analysis": "...", "finalSelection": 7 }
        ```

        It may also appear as a raw JSON string or with escaped new-lines/quotes.
        This helper makes a best-effort attempt to locate the JSON object and
        load it with *json.loads*.  If that fails we fall back to a regex that
        looks for the key/value pair.
        """
        # 1. Try to locate a fenced JSON block first
        fenced = re.search(r"```json\s*(\{.*?\})\s*```", resp, re.DOTALL | re.IGNORECASE)
        json_str: str | None = None
        if fenced:
            json_str = fenced.group(1)
        else:
            # 2. Otherwise grab the first {...} occurrence which is likely the JSON
            braces = re.search(r"\{.*\}", resp, re.DOTALL)
            if braces:
                json_str = braces.group(0)
        if json_str:
            try:
                return json.loads(json_str).get("finalSelection")
            except Exception:
                pass  # fallthrough to regex below
        # 3. Last resort – regex for a standalone number after the key
        m = re.search(r'"finalSelection"\s*[:=]\s*(\d+)', resp)
        return int(m.group(1)) if m else None
    print(response)
    selection = _extract_final_selection(response)
    print(selection)
    print(type(selection))
    if not isinstance(selection, int) or selection < 1:
        selection = 1  # default on parse failure

    # 5. Map the selection number to the correct file regardless of list order
    #    by matching the filename (without extension) with the chosen number.
    from pathlib import Path as _Path

    matched_paths = [p for p in image_paths if _Path(p).stem == str(selection)]
    print([_Path(p).stem for p in image_paths])
    chosen_path = matched_paths[0] if matched_paths else image_paths[0]

    return chosen_path#!/usr/bin/env python3
"""
Save the last frame of an MP4 as a PNG and print the output path.

Usage:
  python save_last_frame.py /path/to/video.mp4
"""
from __future__ import annotations
import sys
from pathlib import Path
import cv2  # type: ignore

def getLastFrame(video_path: str | Path, out_path: str | Path | None = None) -> str:
    p = Path(video_path)
    if not p.exists():
        raise FileNotFoundError(p)

    if out_path is None:
        out_path = p.with_suffix("")  # drop .mp4
        out_path = Path(f"{out_path}_lastframe.png")
    out = Path(out_path)

    cap = cv2.VideoCapture(str(p))
    if not cap.isOpened():
        raise RuntimeError("cv2.VideoCapture failed to open file")

    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if frame_count and frame_count > 0:
        cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, frame_count - 1))
        ok, frame = cap.read()
        if not ok or frame is None:
            ok, frame = _read_to_last(cap)
    else:
        ok, frame = _read_to_last(cap)
    cap.release()

    if not ok or frame is None:
        raise RuntimeError("Could not read last frame via OpenCV")

    if not cv2.imwrite(str(out), frame):
        raise RuntimeError(f"cv2.imwrite failed for {out}")

    return str(out)

def _read_to_last(cap):
    last = None
    ok = False
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        ok, last = ret, frame
    return ok, last
from gemini import ask_gemini
import json
import re


def getMetadata(video_title: str, subideas: list[dict]) -> tuple[str, str]:
    """
    Generate YouTube description and keywords for a video using AI.
    
    Args:
        video_title: The main title of the video
        subideas: List of dictionaries with 'subject' keys representing topics covered
        
    Returns:
        Tuple of (description, keywords_csv)
    """
    subjects = [str(item.get("subject", "")).strip() for item in subideas if isinstance(item, dict)]
    subjects = [s for s in subjects if s]

    prompt = f"""
You are an assistant that writes YouTube metadata for educational videos.

Input:
- Title: {video_title}
- Topics covered (ordered): {', '.join(subjects)}

Task:
- Write a clear, engaging educational description (2-5 sentences) that explains what viewers will learn. Avoid clickbait. Keep it concise and factual. Mention key themes naturally.
- Propose 8-15 short, generic keywords (no hashtags, no duplicates). Keywords should be single words or short phrases relevant to the content and audience discovery.

Output strictly as JSON only, no markdown, using this shape:
{{
  "description": "...",
  "keywords": ["word1", "word2", "word3"]
}}
"""

    def _parse_json_block(text: str) -> dict:
        """Extract JSON from AI response, handling markdown fences."""
        # Try fenced JSON
        m = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL | re.IGNORECASE)
        if m:
            try:
                return json.loads(m.group(1))
            except Exception:
                pass
        # Fallback: first { ... }
        m = re.search(r"\{.*\}", text, re.DOTALL)
        if m:
            try:
                return json.loads(m.group(0))
            except Exception:
                pass
        return {}

    try:
        response = ask_gemini(prompt, model="gemini-2.5-pro")
    except Exception as e:
        print(f"Warning: Failed to generate metadata with AI: {e}")
        response = "{}"

    data = _parse_json_block(response) if isinstance(response, str) else {}
    description = str(data.get("description") or "").strip()
    raw_keywords = data.get("keywords") or []
    
    if isinstance(raw_keywords, str):
        # Split on commas if model returned a string
        raw_keywords = [k.strip() for k in raw_keywords.split(",")]

    # Sanitize keywords: lower noise, dedupe, drop empties
    seen = set()
    keywords: list[str] = []
    for k in raw_keywords:
        if not isinstance(k, str):
            continue
        token = k.strip()
        if not token:
            continue
        key = token.lower()
        if key in seen:
            continue
        seen.add(key)
        keywords.append(token)

    # Fallbacks if AI generation failed
    if not description:
        description = f"An educational overview of '{video_title}', covering: " + ", ".join(subjects[:8]) + "."
    if not keywords:
        keywords = [video_title, "education", "explainer"] + subjects[:7]

    keywords_csv = ", ".join(keywords)
    return description, keywords_csvfrom gemini import ask_gemini
from getImage import getImage

def getSubideas(concept):
    import json
    import re
    
    prompt = f"""
You are an AI that functions as a topic deconstruction tool and image brief generator. Your job is to analyze a given topic and extract 5-10 primary, enumerable subjects directly mentioned or implied by the title, and for each subject provide a concise image search query and goal.

The topic is:
{concept}

YOUR TASK
Return a list of 5-10 core subjects. For each subject, include:
- subject: the subject's name only (no extra words)
- imageSearch: a Google Images–ready query that would surface a representative photo
- goal: one short sentence describing what the image should convey

CRITICAL INSTRUCTIONS
1) Extract, don't brainstorm: Do NOT add related ideas, background, subtopics, or history. Only the items that make up the core subject set.
2) Target 5-10 subjects: Aim for 5-10 subjects total.
3) Preserve order: Keep the natural/canonical or stated order from the title when applicable.
4) Output format: Return a single JSON array of objects. No markdown, no trailing commentary.
5) Subject names: Plain strings only, correctly capitalized; no descriptors, dates, or parentheticals.
6) Focus on concrete objects/things: Extract actual physical objects, places, devices, weapons, vehicles, or tangible entities - NOT abstract concepts, ideas, or processes.
7) imageSearch rules:
   - Include the subject name plus 2–6 neutral descriptors that bias toward real, high-quality photos.
   - Prefer generic terms like photo, scene, landscape, museum, artifact, interior, exterior (where relevant).
   - Use simple tokens only (no quotes or punctuation). Add useful negatives to avoid junk:
     -logo -meme -clipart -infographic -diagram -text
   - Do NOT introduce new entities or time periods not inherent to the subject.
8) goal rules:
   - One clear sentence (8–20 words), imperative voice ("Show …", "Convey …").
   - Describe composition or vibe, not new facts or history.
   - Avoid proper nouns unless inherent to the subject.
9) Try your hardest to use 1-2 words for each subject, you can use acronyms if you need to.

OUTPUT SCHEMA (strict)
[
  {{
    "subject": "String",
    "imageSearch": "String",
    "goal": "String"
  }},
  ...
]

EXAMPLE INPUT
Explaining the four seasons

EXAMPLE OUTPUT
[
  {{
    "subject": "Spring",
    "imageSearch": "Spring photo blooming trees park -logo -meme -clipart -infographic -diagram -text",
    "goal": "Show fresh growth and blossoms to convey renewal and mild weather."
  }},
  {{
    "subject": "Summer",
    "imageSearch": "Summer photo beach sunshine people outdoors -logo -meme -clipart -infographic -diagram -text",
    "goal": "Show bright sun and outdoor activity to convey heat and long days."
  }},
  {{
    "subject": "Autumn",
    "imageSearch": "Autumn photo forest foliage orange red leaves -logo -meme -clipart -infographic -diagram -text",
    "goal": "Show colorful falling leaves to convey cooling weather and harvest time."
  }},
  {{
    "subject": "Winter",
    "imageSearch": "Winter photo snow landscape trees overcast -logo -meme -clipart -infographic -diagram -text",
    "goal": "Show snow and bare trees to convey cold, stillness, and dormancy."
  }}
]"""
    
    response = ask_gemini(prompt, model="gemini-2.5-pro")
    
    # Parse the response flexibly - look for JSON in markdown blocks or plain text
    parsed_data = None
    json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response, re.DOTALL)
    if json_match:
        try:
            parsed_data = json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Try to find JSON array in the response without markdown
    if not parsed_data:
        json_match = re.search(r'(\[.*?\])', response, re.DOTALL)
        if json_match:
            try:
                parsed_data = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
    
    # If no JSON found, raise an error
    if not parsed_data:
        raise ValueError(f"Could not parse JSON from response: {response}")
    
    # Now fetch images for each subject and return simplified array
    results = []
    for item in parsed_data:
        image_path = getImage(item["imageSearch"], item["goal"])
        results.append({
            "subject": item["subject"],
            "image": image_path
        })
    
    return results
import os
import hashlib
import json

CACHE_DIR = "cache/whisper"
os.makedirs(CACHE_DIR, exist_ok=True)

def _cache_path(tts_path):
    """Generate cache path for whisper results based on audio file path."""
    key_src = f"{tts_path}".encode("utf-8")
    return os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".json")

def _load_whisper_cache(cache_path):
    """Load cached whisper results if available."""
    if not os.path.exists(cache_path):
        return None
    try:
        with open(cache_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data["words"]
    except Exception as e:
        print(f"Warning: failed to read whisper cache {cache_path}: {e}")
        return None

def _save_whisper_cache(cache_path, words):
    """Save whisper results to cache."""
    try:
        # Convert Word objects to serializable format
        words_data = []
        for word in words:
            words_data.append({
                "word": word.word,
                "start": word.start,
                "end": word.end
            })
        
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump({"words": words_data}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Warning: failed to write whisper cache {cache_path}: {e}")

def getMediaTimestamps(media, tts_path):
    """Given the media plan (list of dicts) and an audio file path, return the same
    list but with `startTimestamp` and `endTimestamp` (seconds) filled in for each
    entry.

    The audio is transcribed at *word* level using the *small* faster-whisper
    model. For every object we look for an approximate match (fuzzy) of
    `triggerPhrase` and `endPhrase` in the transcript and record the timestamps
    at the beginning of the first matched word and at the end of the last matched
    word respectively.
    """
    from pathlib import Path
    import re, difflib, math

    try:
        from faster_whisper import WhisperModel
    except ImportError as e:
        raise ImportError(
            "faster_whisper is required for getMediaTimestamps. Install with 'pip install faster-whisper'"
        ) from e

    audio_path = Path(tts_path).resolve()  # Use absolute path
    if not audio_path.exists():
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    # Check cache first
    cache_path = _cache_path(str(audio_path))
    cached_words = _load_whisper_cache(cache_path)
    
    if cached_words:
        print(f"Whisper: Using cached results for {audio_path.name}")
        # Convert cached data back to objects with attributes
        class Word:
            def __init__(self, word, start, end):
                self.word = word
                self.start = start
                self.end = end
        
        words = [Word(w["word"], w["start"], w["end"]) for w in cached_words]
    else:
        print(f"Whisper: Processing audio file: {audio_path.name}")
        
        # Load model (small is ~500MB and reasonably fast)
        model = WhisperModel("small", device="cpu", compute_type="int8")

        # Transcribe and collect words across segments
        words = []  # list of Word objects each having .word, .start, .end

        # faster-whisper's transcribe returns (segments_generator, info)
        segments, _ = model.transcribe(str(audio_path), word_timestamps=True)

        for segment in segments:
            # Each segment has a .words attribute which is a list of Word objects
            if not segment.words:
                continue
            words.extend(segment.words)

        if not words:
            raise RuntimeError("No words were produced by the speech recogniser.")
        
        # Save to cache
        _save_whisper_cache(cache_path, words)

    # Normalisation helpers -------------------------------------------------
    _punct = re.compile(r"[^a-z0-9 ]", re.IGNORECASE)

    def _norm(s: str) -> str:
        s = s.lower()
        s = _punct.sub(" ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    # Prepare transcript word list once for quick lookups
    norm_transcript_words = [_norm(w.word) for w in words]

    # Build helper for fuzzy matching ---------------------------------------
    def _best_window_match(phrase_words: list[str], start_idx: int = 0):
        """Return (best_start_index, score) for best fuzzy match of phrase_words
        in transcript starting from start_idx.
        """
        target_str = " ".join(phrase_words)
        best_score = 0.0
        best_i = None
        pl = len(phrase_words)
        # Allow window length ±2 to account for extra/missing words
        for win_len in range(max(1, pl - 2), pl + 3):
            for i in range(start_idx, len(norm_transcript_words) - win_len + 1):
                window = norm_transcript_words[i : i + win_len]
                cand_str = " ".join(window)
                score = difflib.SequenceMatcher(None, target_str, cand_str).ratio()
                if score > best_score:
                    best_score = score
                    best_i = (i, win_len)
                # Early exit if perfect match
                if best_score == 1.0:
                    return best_i[0], best_i[1], best_score
        if best_i is None:
            return None, None, 0.0
        return best_i[0], best_i[1], best_score

    # Iterate media objects and fill timestamps -----------------------------
    for item in media:
        trig_words = _norm(item["triggerPhrase"]).split()
        end_words = _norm(item["endPhrase"]).split()

        trig_start, trig_len, trig_score = _best_window_match(trig_words)
        if trig_start is None:
            # Could not find; skip
            item["startTimestamp"] = None
            item["endTimestamp"] = None
            continue

        # End search begins after trigger start to ensure order
        end_search_start = trig_start + trig_len
        end_start, end_len, end_score = _best_window_match(end_words, start_idx=end_search_start)

        # Compute timestamps
        start_ts = words[trig_start].start  # type: ignore[attr-defined]
        if end_start is not None:
            end_ts = words[end_start + end_len - 1].end  # type: ignore[attr-defined]
        else:
            end_ts = None

        item["startTimestamp"] = round(start_ts, 3) if start_ts is not None else None
        item["endTimestamp"] = round(end_ts, 3) if end_ts is not None else None
        # Optionally store scores for debugging
        item["_matchScores"] = {"trigger": round(trig_score, 3), "end": round(end_score, 3)}

    return media


def get_phrase_timestamps(phrases: list[str], tts_path: str) -> dict[str, float]:
    """
    Convenience wrapper around getMediaTimestamps for a simpler use-case:
    Given a list of phrases and the path to the corresponding audio, return a
    mapping {phrase: start_timestamp_seconds}.
    """
    if not phrases:
        return {}

    # Re-use the existing implementation by constructing a minimal media plan
    media_plan = [{"triggerPhrase": p, "endPhrase": ""} for p in phrases]
    results = getMediaTimestamps(media_plan, tts_path)
    return {item["triggerPhrase"]: item.get("startTimestamp") for item in results}# import os
# import hashlib
# import requests
# import urllib.parse

# CACHE_DIR = "cache/openai_tts"
# os.makedirs(CACHE_DIR, exist_ok=True)

# def _cache_path(text, voice="ash", prompt="british, youtube commentary style, expressive, uses pauses effectively"):
#     key_src = f"{voice}|{prompt}|{text}".encode("utf-8")
#     # Use .mp3 since the service returns MP3 content; extension mismatch can confuse probes
#     return os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".mp3")

# def getTTS(text, voice="ash", prompt="british, youtube commentary style, expressive, uses pauses effectively"):
#     text = text.strip()
#     if not text:
#         raise ValueError("getOpenAiTTS: text is empty.")

#     print(f"OpenAI TTS: Processing text: {text[:80]!r}...")  # trim for logs

#     cache_path = _cache_path(text, voice=voice, prompt=prompt)
    
#     if os.path.exists(cache_path):
#         return cache_path

#     # Backward-compat: migrate old .wav caches that actually contain MP3
#     legacy_path = cache_path[:-4] + '.wav'
#     if os.path.exists(legacy_path):
#         try:
#             os.replace(legacy_path, cache_path)
#             return cache_path
#         except Exception:
#             pass

#     # URL encode the parameters
#     encoded_text = urllib.parse.quote_plus(text)
#     encoded_prompt = urllib.parse.quote_plus(prompt)
#     encoded_voice = urllib.parse.quote_plus(voice)
    
#     url = f"https://www.openai.fm/api/generate?input={encoded_text}&prompt={encoded_prompt}&voice={encoded_voice}"
    
#     try:
#         response = requests.get(url, stream=True)
#         response.raise_for_status()
        
#         with open(cache_path, "wb") as f:
#             for chunk in response.iter_content(chunk_size=8192):
#                 f.write(chunk)
                
#     except Exception as e:
#         raise RuntimeError(f"OpenAI TTS request failed: {e}") from e

#     return cache_path
# import os, hashlib
# # from gradio_client import Client

# # CACHE_DIR = "cache/tts"
# # os.makedirs(CACHE_DIR, exist_ok=True)

# # def _cache_path(text, voice="en-GB-RyanNeural - en-GB (Male)", rate=0, pitch=0):
# #     key_src = f"{voice}|{rate}|{pitch}|{text}".encode("utf-8")
# #     return os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".wav")

# # def getTTS(text, voice="en-GB-RyanNeural - en-GB (Male)", rate=0, pitch=0):
# #     text = text.strip()
# #     if not text:
# #         raise ValueError("getTTS: text is empty.")

# #     print(f"TTS: Processing text: {text[:80]!r}...")  # trim for logs

# #     cache_path = _cache_path(text, voice=voice, rate=rate, pitch=pitch)
    
# #     if os.path.exists(cache_path):
# #         return cache_path

# #     # call gradio client
# #     try:
# #         client = Client("keshav6936/GENAI-TTS-Text-to-Speech")
# #         result = client.predict(
# #             text=text,
# #             voice=voice,
# #             rate=rate,
# #             pitch=pitch,
# #             api_name="/predict"
# #         )
# #     except Exception as e:
# #         raise RuntimeError(f"TTS request failed: {e}") from e

# #     if not result or len(result) < 2:
# #         raise RuntimeError(f"Unexpected TTS result: {result}")

# #     audio_file_path, status = result
    
# #     if not audio_file_path:
# #         raise RuntimeError(f"TTS generation failed: {status}")

# #     # copy to cache
# #     import shutil
# #     shutil.copy2(audio_file_path, cache_path)

# #     return cache_path

import os, hashlib
from gradio_client import Client

CACHE_DIR = "cache/tts"
os.makedirs(CACHE_DIR, exist_ok=True)

def _append_log(message: str) -> None:
    """Append a timestamped log line to log.txt in project root."""
    from datetime import datetime
    try:
        with open("log.txt", "a", encoding="utf-8") as f:
            f.write(f"[{datetime.now().isoformat()}] {message}\n")
    except Exception:
        # Avoid raising in logging path; best effort only
        pass


def getTTS(text, voice="Liam", previous_text=None):
    import requests
    import json
    import time
    from dotenv import load_dotenv
    
    load_dotenv()
    
    text = text.strip()
    if not text:
        raise ValueError("getTTS: text is empty.")

    print(f"TTS: Processing text: {text[:80]!r}...")

    # Update cache path for new parameters
    key_src = f"{voice}|{previous_text or ''}|{text}".encode("utf-8")
    cache_path = os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".mp3")
    
    if os.path.exists(cache_path):
        print(f"TTS: Using cached audio: {cache_path}")
        return cache_path

    # Primary (and only): ElevenLabs via FAL with retries (3)
    api_key = os.getenv('FAL_KEY')
    if not api_key:
        _append_log("TTS: FAL_KEY not set; cannot proceed")
        import sys
        sys.exit(1)

    url = "https://fal.run/fal-ai/elevenlabs/tts/turbo-v2.5"
    headers = {
        "Authorization": f"Key {api_key}",
        "Content-Type": "application/json"
    }

    payload = {
        "text": text,
        "voice": voice,
        "stability": 1,
    }
    if previous_text:
        payload["previous_text"] = previous_text

    max_retries = 3
    last_error_msg = None
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            status = response.status_code
            if status >= 400:
                # Log body on errors for visibility
                body = None
                try:
                    body = response.text[:1000]
                except Exception:
                    body = "<failed to read body>"
                last_error_msg = f"HTTP {status} from FAL; body: {body}"
                _append_log(f"TTS: attempt {attempt+1}/{max_retries} failed — {last_error_msg}")
                # Do not raise_for_status yet; continue retry loop
            else:
                result = response.json()
                audio_url = result.get("audio", {}).get("url")
                if not audio_url:
                    last_error_msg = "No audio URL returned from API"
                    _append_log(f"TTS: attempt {attempt+1}/{max_retries} failed — {last_error_msg}")
                else:
                    # Download audio with timeout
                    audio_response = requests.get(audio_url, timeout=60)
                    audio_response.raise_for_status()
                    with open(cache_path, "wb") as f:
                        f.write(audio_response.content)
                    return cache_path
        except Exception as e:
            last_error_msg = str(e)
            _append_log(f"TTS: attempt {attempt+1}/{max_retries} exception — {last_error_msg}")
        # Small delay between attempts
        time.sleep(2)

    # All retries failed — log context and exit program
    snippet = text[:200].replace("\n", " ")
    _append_log(f"TTS: all retries failed; voice={voice}, prev_len={(len(previous_text) if previous_text else 0)}, text_len={len(text)}, text_snippet={snippet!r}, error={last_error_msg}")
    import sys
    sys.exit(1)
from runit import runit

runit("profiles/govlist")# make_all_ideas_image.py
# pip install pillow moviepy

from PIL import Image, ImageDraw, ImageFont, ImageFilter
import os
from typing import List, Dict, Tuple
import numpy as np
from moviepy import ImageSequenceClip, AudioClip
import math
from getTTS import getTTS
from getAudioLength import getAudioLength
from overlayAudioVideo import overlayAudioVideo

# ---------------------- constants ----------------------
MAX_IDEA_SIZE = 300  # Maximum diameter for each idea circle
# Label rendering constraints
LABEL_MAX_LINES = 2
LABEL_MARGIN_X = 12  # inner horizontal padding inside the label area
LABEL_MARGIN_Y = 6   # inner vertical padding inside the label area
MAX_FONT_SIZE = 48   # Maximum font size for labels

# ---------------------- helpers ----------------------

def _cover_fit(bg: Image.Image, target_size: Tuple[int, int]) -> Image.Image:
    """Resize+crop a background to cover target_size (like CSS background-size: cover)."""
    tw, th = target_size
    bw, bh = bg.size
    scale = max(tw / bw, th / bh)
    new_size = (int(bw * scale), int(bh * scale))
    bg = bg.resize(new_size, Image.LANCZOS)
    # center crop
    left = (bg.width - tw) // 2
    top = (bg.height - th) // 2
    return bg.crop((left, top, left + tw, top + th))

def _load_font(preferred: str, size: int, font_path: str = "font.ttf") -> ImageFont.FreeTypeFont:
    """Try a few common fonts; fall back gracefully."""
    candidates = [
        preferred,
        font_path,
        "DejaVuSans-Bold.ttf",
        "/System/Library/Fonts/Supplemental/Arial Bold.ttf",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
        "Arial.ttf",
    ]
    for path in candidates:
        try:
            return ImageFont.truetype(path, size)
        except Exception:
            continue
    return ImageFont.load_default()

def _fit_font(draw: ImageDraw.ImageDraw, text: str, max_width: int, start_px: int, font_path: str = "font.ttf") -> ImageFont.FreeTypeFont:
    """Shrink font so text fits within max_width."""
    size = min(start_px, MAX_FONT_SIZE)
    while size > 8:
        font = _load_font("", size, font_path)
        w, _ = draw.textbbox((0, 0), text, font=font)[2:]
        if w <= max_width:
            return font
        size -= 1
    return _load_font("", 8, font_path)

def _wrap_text_to_width(draw: ImageDraw.ImageDraw,
                        text: str,
                        font: ImageFont.FreeTypeFont,
                        max_width: int,
                        max_lines: int = 3) -> Tuple[List[str], int, int]:
    """Greedy wrap text into lines that fit max_width. Returns (lines, block_w, block_h).

    - Splits on spaces; if a single token is too wide, it is hard-wrapped by characters.
    - Up to max_lines lines are produced; callers should shrink font if more would be needed.
    """
    if not text:
        return [""], 0, 0

    words = text.split()
    lines: List[str] = []
    current = ""

    def text_size(s: str) -> Tuple[int, int]:
        box = draw.textbbox((0, 0), s, font=font)
        return box[2] - box[0], box[3] - box[1]

    i = 0
    while i < len(words):
        word = words[i]
        # If a single word is wider than the box, hard-wrap it by characters
        w_word, _ = text_size(word)
        if w_word > max_width and len(word) > 1:
            # Flush current line first
            if current:
                lines.append(current)
                current = ""
                if len(lines) >= max_lines:
                    break
            # Hard-wrap this long token
            start = 0
            while start < len(word):
                # Fit as many characters as possible on this line
                end = start + 1
                last_good = start + 1
                while end <= len(word):
                    segment = word[start:end]
                    seg_w, _ = text_size(segment)
                    if seg_w <= max_width:
                        last_good = end
                        end += 1
                    else:
                        break
                segment = word[start:last_good]
                lines.append(segment)
                start = last_good
                if len(lines) >= max_lines:
                    break
            i += 1
            continue

        # Try to place word on current line
        trial = word if not current else current + " " + word
        if text_size(trial)[0] <= max_width:
            current = trial
            i += 1
        else:
            lines.append(current or word)
            current = "" if current else ""
            if not current:
                # If we placed the word as its own line, move to next
                if trial == word:
                    i += 1
            if len(lines) >= max_lines:
                break

    if current and len(lines) < max_lines:
        lines.append(current)

    # Measure block size
    max_w = 0
    line_h = draw.textbbox((0, 0), "Ag", font=font)[3]
    spacing = max(4, int(line_h * 0.15))
    for ln in lines:
        lw = draw.textbbox((0, 0), ln, font=font)[2]
        if lw > max_w:
            max_w = lw
    block_h = len(lines) * line_h + max(0, len(lines) - 1) * spacing
    return lines, max_w, block_h

def _fit_wrapped_text(draw: ImageDraw.ImageDraw,
                      text: str,
                      max_width: int,
                      max_height: int,
                      start_px: int,
                      max_lines: int = 3,
                      font_path: str = "font.ttf") -> Tuple[ImageFont.FreeTypeFont, List[str], int, int, int]:
    """Find the largest font size so the wrapped text fits within (max_width, max_height).

    Returns: (font, lines, block_w, block_h, line_height)
    """
    size = min(start_px, MAX_FONT_SIZE)
    while size > 8:
        font = _load_font("", size, font_path)
        lines, block_w, block_h = _wrap_text_to_width(draw, text, font, max_width, max_lines=max_lines)
        line_h = draw.textbbox((0, 0), "Ag", font=font)[3]
        if block_w <= max_width and block_h <= max_height and len(lines) <= max_lines:
            return font, lines, block_w, block_h, line_h
        size -= 1
    # Fallback tiny font
    font = _load_font("", 8, font_path)
    lines, block_w, block_h = _wrap_text_to_width(draw, text, font, max_width, max_lines=max_lines)
    line_h = draw.textbbox((0, 0), "Ag", font=font)[3]
    return font, lines, block_w, block_h, line_h

def _find_uniform_font_size(draw: ImageDraw.ImageDraw,
                            labels: List[str],
                            max_width: int,
                            max_height: int,
                            start_px: int,
                            max_lines: int,
                            font_path: str = "font.ttf") -> ImageFont.FreeTypeFont:
    """Find one font size that allows all labels to fit (wrapped) in the box.

    Chooses the largest size that satisfies constraints for every label.
    """
    size = min(start_px, MAX_FONT_SIZE)
    while size > 8:
        font = _load_font("", size, font_path)
        ok = True
        for label in labels:
            lines, block_w, block_h = _wrap_text_to_width(draw, label, font, max_width, max_lines)
            if block_w > max_width or block_h > max_height or len(lines) > max_lines:
                ok = False
                break
        if ok:
            return font
        size -= 1
    return _load_font("", 8, font_path)

def _circle_thumb(img_path: str, diameter: int) -> Image.Image:
    """Open an image, center-crop to square, resize, and mask to a circle with border."""
    # Robust open with fallback
    try:
        img = Image.open(img_path).convert("RGB")
    except Exception:
        # fallback placeholder
        img = Image.new("RGB", (diameter, diameter), (40, 40, 40))
    # center-crop to square
    w, h = img.size
    side = min(w, h)
    left = (w - side) // 2
    top = (h - side) // 2
    img = img.crop((left, top, left + side, top + side)).resize((diameter, diameter), Image.LANCZOS)

    # circular mask
    mask = Image.new("L", (diameter, diameter), 0)
    draw = ImageDraw.Draw(mask)
    draw.ellipse((0, 0, diameter - 1, diameter - 1), fill=255)

    # simplified without vignette

    img = Image.composite(img, Image.new("RGB", (diameter, diameter), (20, 20, 20)), mask)
    mimg = Image.merge("RGBA", (*img.split(), mask))
    # high-quality anti-aliased border ring
    scale = 4  # render at 4x for anti-aliasing
    ring_size = diameter * scale
    ring = Image.new("RGBA", (ring_size, ring_size), (0, 0, 0, 0))
    rd = ImageDraw.Draw(ring)
    border = max(8, (diameter * scale) // 60)  # scale border thickness too
    rd.ellipse((0, 0, ring_size - 1, ring_size - 1),
               outline=(0, 0, 0, 255), width=border)
    # scale back down with high-quality resampling for smooth edges
    ring = ring.resize((diameter, diameter), Image.LANCZOS)

    composed = Image.alpha_composite(mimg, ring)
    return composed



def _balanced_layout(n: int, W: int, H: int, pad: int) -> Tuple[int, List[int], int, int]:
    """
    Choose row count and per-row column counts that maximize circle diameter.
    Returns: rows, cols_per_row, cell_w, cell_h (approx for largest-row calc).
    """
    inner_w, inner_h = W - 2 * pad, H - 2 * pad
    best = None

    for rows in range(1, n + 1):
        # gutters proportional to canvas
        gutter_x = max(20, min(inner_w // 40, 60))     # ~2.5% width
        gutter_y = max(24, min(inner_h // 25, 70))     # ~4% height

        base = n // rows
        rem = n % rows
        cols_per_row = [(base + 1 if i < rem else base) for i in range(rows)]
        c_max = max(cols_per_row)

        cell_w = (inner_w - (c_max - 1) * gutter_x) // c_max
        cell_h = (inner_h - (rows - 1) * gutter_y) // rows

        # reserve space ~28% for label; circle diam limited by both w and h
        label_frac = 0.28
        diameter = min(cell_w, int(cell_h * (1 - label_frac)))
        # Apply maximum size constraint
        diameter = min(diameter, MAX_IDEA_SIZE)

        if diameter <= 0:
            continue
        score = diameter  # maximize circle diameter
        if best is None or score > best[0]:
            best = (score, rows, cols_per_row, cell_w, cell_h, gutter_x, gutter_y)

    if best is None:
        # degenerate fallback
        return 1, [n], inner_w, inner_h
    _, rows, cols_per_row, cell_w, cell_h, gx, gy = best
    return rows, cols_per_row, cell_w, cell_h

# ---------------------- main function ----------------------

def makeAllIdeasImage(items: List[Dict[str, str]],
                      output_path: str = "all_ideas.png",
                      size: Tuple[int, int] = (1920, 1080),
                      background_path: str = "background.png",
                      font_path: str = "font.ttf") -> Image.Image:
    """
    Render a polished 16:9 collage.
    items: [{'subject': str, 'image': str}, ...]
    Returns the PIL image and also saves to output_path.
    """
    W, H = size

    # Background
    if os.path.exists(background_path):
        bg = Image.open(background_path).convert("RGB")
        canvas = _cover_fit(bg, (W, H)).convert("RGBA")
    else:
        # tasteful fallback gradient
        grad = Image.new("L", (1, H), color=0)
        for y in range(H):
            grad.putpixel((0, y), int(30 + 70 * y / H))
        grad = grad.resize((W, H))
        canvas = Image.merge("RGBA",
                             (Image.new("L", (W, H), 18),
                              Image.new("L", (W, H), 18),
                              Image.new("L", (W, H), 22),
                              grad))

    draw = ImageDraw.Draw(canvas)

    # Layout
    pad = max(40, min(W, H) // 16)  # ~6% edge padding
    rows, cols_per_row, cell_w, cell_h = _balanced_layout(len(items), W, H, pad)
    inner_w, inner_h = W - 2 * pad, H - 2 * pad
    gutter_x = max(20, min(inner_w // 40, 60))
    gutter_y = max(24, min(inner_h // 25, 70))
    label_frac = 0.28
    diameter = min(cell_w, int(cell_h * (1 - label_frac)))
    # Apply maximum size constraint
    diameter = min(diameter, MAX_IDEA_SIZE)
    label_area = max(24, cell_h - diameter)

    # Fonts
    tmp_font = _load_font("", max(24, diameter // 3), font_path)
    # Compute a single uniform font size for all labels (with wrapping)
    labels_all = [str(it.get("subject", "")).strip() for it in items]
    avail_w = max(10, int(cell_w * 0.95) - 2 * LABEL_MARGIN_X)
    avail_h = max(8, label_area - 2 * LABEL_MARGIN_Y)
    start_px = max(24, avail_h)
    uniform_font = _find_uniform_font_size(draw, labels_all, avail_w, avail_h, start_px, LABEL_MAX_LINES, font_path)
    uniform_line_h = draw.textbbox((0, 0), "Ag", font=uniform_font)[3]
    uniform_spacing = max(4, int(uniform_line_h * 0.15))

    # Calculate total grid height for vertical centering
    total_grid_height = rows * cell_h + (rows - 1) * gutter_y
    
    # Center the grid vertically
    grid_start_y = pad + (inner_h - total_grid_height) // 2
    
    # Prepare list to capture exact circle centers for later zooming
    circle_centers: List[Tuple[int, int]] = []
    layout_info: List[Dict[str, int]] = []

    # Render rows
    idx = 0
    y_cursor = grid_start_y
    for r in range(rows):
        cols = cols_per_row[r]
        row_w = cols * cell_w + (cols - 1) * gutter_x
        x_start = pad + (inner_w - row_w) // 2  # center the shorter rows

        for c in range(cols):
            if idx >= len(items):
                break
            item = items[idx]
            idx += 1

            # positions inside this cell
            cell_x = x_start + c * (cell_w + gutter_x)
            cell_y = y_cursor
            circle_x = cell_x + (cell_w - diameter) // 2

            # label text using a uniform font size and wrapping for consistency
            label = str(item.get("subject", "")).strip()
            lines, block_w, block_h = _wrap_text_to_width(
                draw, label, uniform_font, avail_w, max_lines=LABEL_MAX_LINES
            )

            # vertically center the circle + label as a group within the cell
            text_gap = max(8, int(diameter * 0.08))
            group_h = diameter + text_gap + block_h
            base_y = cell_y + max(0, (cell_h - group_h) // 2)
            circle_y = base_y
            label_y = base_y + diameter + text_gap

            # draw circle
            circ = _circle_thumb(item.get("image", ""), diameter)
            canvas.alpha_composite(circ, (circle_x, circle_y))

            # store exact metadata for zoom
            circle_center_x = circle_x + diameter // 2
            circle_center_y = circle_y + diameter // 2
            circle_centers.append((circle_center_x, circle_center_y))
            group_center_y = base_y + group_h // 2
            cell_center_x = cell_x + cell_w // 2
            layout_info.append({
                "circle_cx": circle_center_x,
                "circle_cy": circle_center_y,
                "group_cy": group_center_y,
                "circle_d": diameter,
                "cell_cx": cell_center_x,
                "cell_y": cell_y,
                "cell_h": cell_h
            })

            # draw label (centered, multi-line) with inner margins
            tx = cell_x + (cell_w - min(block_w, avail_w)) // 2
            cy = label_y + LABEL_MARGIN_Y
            for ln in lines:
                lw = draw.textbbox((0, 0), ln, font=uniform_font)[2]
                lx = cell_x + (cell_w - lw) // 2
                draw.text((lx, cy), ln, font=uniform_font, fill=(0, 0, 0, 255))
                cy += uniform_line_h + uniform_spacing

        y_cursor += cell_h + gutter_y

    
    canvas.save(output_path)
    # attach centers and layout info to the image for consumers (non-breaking API)
    try:
        setattr(canvas, "_circle_centers", circle_centers)
        setattr(canvas, "_layout_info", layout_info)
    except Exception:
        pass
    return canvas

# ---------------------- zoom video function ----------------------

# Animation constants
ZOOM_DURATION = 2.0  # seconds (minimum animation length baseline)
ZOOM_FPS = 30
ZOOM_SPEED = 2.0  # zoom factor per second
FINAL_SCALE = 2.0  # how much to zoom into the final item (reduced to keep full item visible)
PAUSE_START = 0.5  # seconds to pause at start (set to 0 to keep base at 2s)
PAUSE_END = 0.0  # seconds to pause at end (set to 0 to keep base at 2s)

def zoomintoidea(items: List[Dict[str, str]], 
                 index: int,
                 output_path: str = "zoom_video.mp4",
                 size: Tuple[int, int] = (1920, 1080),
                 background_path: str = "background.png",
                 font_path: str = "font.ttf") -> str:
    """
    Create a video that zooms into a specific item from the grid.
    
    Args:
        items: List of items with 'subject' and 'image' keys
        index: Index of the item to zoom into
        output_path: Path for output video file
        size: Video resolution
        background_path: Background image path
        
    Returns:
        Path to the created video file
    """
    if index < 0 or index >= len(items):
        raise ValueError(f"Index {index} out of range for {len(items)} items")
    
    W, H = size
    total_frames = int((ZOOM_DURATION + PAUSE_START + PAUSE_END) * ZOOM_FPS)
    pause_start_frames = int(PAUSE_START * ZOOM_FPS)
    pause_end_frames = int(PAUSE_END * ZOOM_FPS)
    zoom_frames = total_frames - pause_start_frames - pause_end_frames
    
    frames = []
    
    # Generate TTS for the idea name and measure duration
    idea_label = str(items[index].get("subject", f"item_{index}")).strip() or f"item_{index}"
    tts_path = getTTS(idea_label)
    audio_len = getAudioLength(tts_path)
    end_linger = 0.35  # slight delay to avoid a rushed cut when audio runs long

    # Create the full grid image first
    full_image = makeAllIdeasImage(items, "temp_grid.png", size, background_path, font_path)

    # If the image carries exact circle centers, use them directly
    circle_centers = getattr(full_image, "_circle_centers", None)
    if isinstance(circle_centers, list) and 0 <= index < len(circle_centers):
        target_x, target_y = circle_centers[index]
        # Also try to get full layout info for precise fallback calculations later if needed
        layout_info = getattr(full_image, "_layout_info", None)
    else:
        target_x = target_y = None
    
    # Calculate the target item position (fallback if centers not available)
    pad = max(40, min(W, H) // 16)
    rows, cols_per_row, cell_w, cell_h = _balanced_layout(len(items), W, H, pad)
    inner_w, inner_h = W - 2 * pad, H - 2 * pad
    gutter_x = max(20, min(inner_w // 40, 60))
    gutter_y = max(24, min(inner_h // 25, 70))
    
    # Calculate total grid height for vertical centering (same as main function)
    total_grid_height = rows * cell_h + (rows - 1) * gutter_y
    grid_start_y = pad + (inner_h - total_grid_height) // 2
    
    # Find the target item's position (center of entire cell including text)
    idx = 0
    y_cursor = grid_start_y
    if target_x is None or target_y is None:
        target_x = target_y = 0
    
    for r in range(rows):
        cols = cols_per_row[r]
        row_w = cols * cell_w + (cols - 1) * gutter_x
        x_start = pad + (inner_w - row_w) // 2
        
        for c in range(cols):
            if idx == index and (circle_centers is None):
                # Compute the exact circle center using the same sizing rules as rendering
                label_frac = 0.28
                diameter = min(cell_w, int(cell_h * (1 - label_frac)))
                diameter = min(diameter, MAX_IDEA_SIZE)
                label_w_max = int(cell_w * 0.95)
                font_start_px = max(24, cell_h - diameter)
                measure_draw = ImageDraw.Draw(Image.new("RGBA", (8, 8)))
                label = str(items[idx].get("subject", "")).strip()
                # Use same uniform rules as renderer to compute group height
                avail_w = max(10, int(cell_w * 0.95) - 2 * LABEL_MARGIN_X)
                avail_h = max(24, cell_h - diameter - 2 * LABEL_MARGIN_Y)
                # Estimate with a conservative font size similar to the renderer
                est_font = _find_uniform_font_size(measure_draw, [label], avail_w, avail_h, font_start_px, LABEL_MAX_LINES, font_path)
                lines, block_w, block_h = _wrap_text_to_width(measure_draw, label, est_font, avail_w, max_lines=LABEL_MAX_LINES)
                text_gap = max(8, int(diameter * 0.08))
                group_h = diameter + text_gap + block_h + 2 * LABEL_MARGIN_Y
                base_y = y_cursor + max(0, (cell_h - group_h) // 2)

                target_x = x_start + c * (cell_w + gutter_x) + cell_w // 2
                # Focus on the circle center, not the group center
                target_y = base_y + diameter // 2
                break
            idx += 1
            if idx >= len(items):
                break
        if idx == index:
            break
        y_cursor += cell_h + gutter_y
    
    # Generate frames
    for frame_num in range(total_frames):
        if frame_num < pause_start_frames:
            # Pause at start - show full grid
            scale = 1.0
            center_x, center_y = W // 2, H // 2
        elif frame_num >= total_frames - pause_end_frames:
            # Pause at end - show zoomed view
            scale = FINAL_SCALE
            center_x, center_y = target_x, target_y
        else:
            # Zoom animation - direct linear interpolation
            progress = (frame_num - pause_start_frames) / zoom_frames
            # Simple smooth easing - less dramatic than cubic
            progress = progress * progress * (3 - 2 * progress)  # smoothstep
            
            scale = 1.0 + (FINAL_SCALE - 1.0) * progress
            center_x = W // 2 + (target_x - W // 2) * progress
            center_y = H // 2 + (target_y - H // 2) * progress
        
        # Create frame by cropping and scaling
        frame = full_image.copy()
        
        # Calculate crop region
        crop_w = W / scale
        crop_h = H / scale
        
        # Calculate initial crop bounds
        left = center_x - crop_w / 2
        top = center_y - crop_h / 2
        right = left + crop_w
        bottom = top + crop_h
        
        # Adjust if we hit boundaries
        if left < 0:
            right = right - left
            left = 0
        if top < 0:
            bottom = bottom - top
            top = 0
        if right > W:
            left = left - (right - W)
            right = W
        if bottom > H:
            top = top - (bottom - H)
            bottom = H
            
        # Ensure we don't go negative after adjustments
        left = max(0, left)
        top = max(0, top)
        right = min(W, right)
        bottom = min(H, bottom)
        
        # Ensure valid crop coordinates
        if right <= left:
            left = 0
            right = W
        if bottom <= top:
            top = 0
            bottom = H
        
        # Crop and resize
        cropped = frame.crop((int(left), int(top), int(right), int(bottom)))
        frame_resized = cropped.resize((W, H), Image.LANCZOS)
        
        # Convert to RGB for video
        frame_rgb = frame_resized.convert('RGB')
        frames.append(np.array(frame_rgb))
    
    # If audio is longer than animation, hold the last frame until audio completes (+ small linger)
    base_duration = len(frames) / ZOOM_FPS
    target_min_duration = audio_len + end_linger
    if target_min_duration > base_duration and frames:
        extra_seconds = target_min_duration - base_duration
        extra_frames = int(math.ceil(extra_seconds * ZOOM_FPS))
        if extra_frames > 0:
            frames.extend([frames[-1]] * extra_frames)

    # Create video with silent audio to ensure consistent concat later
    clip = ImageSequenceClip(frames, fps=ZOOM_FPS)
    # Use 48kHz silent audio to match the pipeline and avoid resample artifacts later
    silent_audio = AudioClip(lambda t: 0.0, duration=clip.duration, fps=48000)
    clip = clip.with_audio(silent_audio)
    clip.write_videofile(
        output_path,
        codec='libx264',
        audio_codec='aac',
        audio=True,
        fps=ZOOM_FPS,
        bitrate=None,
        audio_bitrate='192k',
        temp_audiofile=None,
        logger=None
    )

    # Overlay the TTS audio on the zoom video (video already extended to cover audio)
    overlayAudioVideo(output_path, tts_path, trim_to_shortest=False)
    
    # Clean up temp file
    if os.path.exists("temp_grid.png"):
        os.remove("temp_grid.png")
    
    return output_path

# ---------------------- example usage ----------------------
if __name__ == "__main__":
    sample = [
        {'subject': 'DEFCON 5', 'image': r'images\DEFCON+5+military+normal+operations+photo+peacetime+-logo+-meme+-clipart+-infographic+-diagram+-text\5.jpg'},
        {'subject': 'DEFCON 4', 'image': r'images\DEFCON+4+military+intelligence+gathering+operations+photo+-logo+-meme+-clipart+-infographic+-diagram+-text\8.jpg'},
        {'subject': 'DEFCON 3', 'image': r'images\DEFCON+3+military+heightened+readiness+alert+state+photo+-logo+-meme+-clipart+-infographic+-diagram+-text\9.jpg'},
        {'subject': 'DEFCON 2', 'image': r'images\DEFCON+2+military+urgent+readiness+preparation+imminent+threat+photo+-logo+-meme+-clipart+-infographic+-diagram+-text\5.jpg'},
        {'subject': 'DEFCON 1', 'image': r'images\DEFCON+1+military+maximum+readiness+active+combat+photo+-logo+-meme+-clipart+-infographic+-diagram+-text\9.jpg'},
    {'subject': 'DEFCON 9', 'image': r'images\DEFCON+5+military+normal+operations+photo+peacetime+-logo+-meme+-clipart+-infographic+-diagram+-text\5.jpg'},
        {'subject': 'DEFCON 4', 'image': r'images\DEFCON+4+military+intelligence+gathering+operations+photo+-logo+-meme+-clipart+-infographic+-diagram+-text\8.jpg'},
        {'subject': 'DEFCON 3', 'image': r'images\DEFCON+3+military+heightened+readiness+alert+state+photo+-logo+-meme+-clipart+-infographic+-diagram+-text\9.jpg'},
        {'subject': 'DEFCON 2', 'image': r'images\DEFCON+2+military+urgent+readiness+preparation+imminent+threat+photo+-logo+-meme+-clipart+-infographic+-diagram+-text\5.jpg'},
        {'subject': 'DEFCON 1', 'image': r'images\DEFCON+1+military+maximum+readiness+active+combat+photo+-logo+-meme+-clipart+-infographic+-diagram+-text\9.jpg'},
    
    ]
    # makeAllIdeasImage(sample, output_path="all_ideas.png", size=(1920, 1080), background_path="background.png")
    
    # Example: Create zoom video for DEFCON 1 (index 4)
    zoomintoidea(sample, 5, "defcon1_zoom.mp4", size=(1920, 1080), background_path="background.png", font_path="font.ttf")
from combineVideos import combineVideos
from create9x16Video import create9x16Video
from captions import add_tiktok_captions
from getTTS import getTTS
from upload_video import publish_short

import os
import re
import subprocess

def createEndClip(image_path: str, tts_text: str, output_path: str, voice: str = "Liam") -> str:
    """
    Create a video clip from a 9:16 image with TTS audio.
    
    Args:
        image_path: Path to the 9:16 image (shortend.png)
        tts_text: Text to convert to speech
        output_path: Path for the output video clip
        voice: Voice to use for TTS
    
    Returns:
        Path to the created video clip
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    
    # Generate TTS audio
    print(f"Generating TTS for: '{tts_text}'")
    audio_path = getTTS(tts_text, voice=voice)
    
    # Get audio duration to match video length
    duration_cmd = [
        'ffprobe', '-v', 'error', '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1', audio_path
    ]
    
    try:
        result = subprocess.run(duration_cmd, capture_output=True, text=True, check=True)
        audio_duration = float(result.stdout.strip())
    except Exception as e:
        print(f"Could not get audio duration, using default 3 seconds: {e}")
        audio_duration = 3.0
    
    # Create output directory if needed
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # Create video from image + audio
    cmd = [
        'ffmpeg',
        '-hide_banner', '-loglevel', 'error',
        '-loop', '1',
        '-i', image_path,
        '-i', audio_path,
        '-c:v', 'libx264',
        '-t', str(audio_duration),
        '-pix_fmt', 'yuv420p',
        '-c:a', 'aac',
        '-shortest',
        '-y',
        output_path
    ]
    
    print(f"Creating end clip: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        if result.stderr:
            print(f"FFmpeg stderr: {result.stderr}")
        return output_path
    except subprocess.CalledProcessError as e:
        print(f"End clip creation failed: {e.returncode}, stderr: {e.stderr}")
        raise RuntimeError(f"Failed to create end clip: {e.stderr}")

def speedUpVideo(input_path: str, output_path: str, speed_multiplier: float = 1.2) -> str:
    """
    Speed up a video by the given multiplier while preserving audio pitch.
    
    Args:
        input_path: Path to input video
        output_path: Path for output video
        speed_multiplier: Speed factor (e.g., 1.2 = 20% faster, 1.5 = 50% faster)
    
    Returns:
        Path to the sped-up video
    """
    if not os.path.exists(input_path):
        raise FileNotFoundError(f"Input video not found: {input_path}")
    
    # Create output directory if needed
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # ffmpeg command to speed up video and audio while preserving pitch
    cmd = [
        'ffmpeg',
        '-hide_banner', '-loglevel', 'error',
        '-i', input_path,
        '-filter_complex',
        f'[0:v]setpts=PTS/{speed_multiplier}[v];[0:a]atempo={speed_multiplier}[a]',
        '-map', '[v]',
        '-map', '[a]',
        '-c:v', 'libx264',
        '-c:a', 'aac',
        '-y',
        output_path
    ]
    
    print(f"Speeding up video by {speed_multiplier}x: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        if result.stderr:
            print(f"FFmpeg stderr: {result.stderr}")
        return output_path
    except subprocess.CalledProcessError as e:
        print(f"Speed adjustment failed: {e.returncode}, stderr: {e.stderr}")
        raise RuntimeError(f"Failed to speed up video: {e.stderr}")

def makeAndUploadShort(segment, video_idea, subidea, assetspath, speed_multiplier: float = 2):
    print(f"Processing segment: {segment} (speed: {speed_multiplier}x)")
    
    # Create 9:16 format video with blurred background
    segment_id = os.path.splitext(os.path.basename(segment))[0]
    temp_9x16_filename = f"temp_9x16_{segment_id}.mp4"
    temp_9x16_path = os.path.join("cache", "shorts", temp_9x16_filename)
    
    # Ensure shorts directory exists
    os.makedirs(os.path.dirname(temp_9x16_path), exist_ok=True)
    
    # Step 1: Convert to 9:16 format
    nine_sixteen_video = create9x16Video(segment, temp_9x16_path)
    print(f"Created 9:16 video: {nine_sixteen_video}")
    
    # Step 2: Create end clip with shortend.png + TTS
    segment_id = os.path.splitext(os.path.basename(segment))[0]
    end_clip_filename = f"temp_endclip_{segment_id}.mp4"
    end_clip_path = os.path.join("cache", "shorts", end_clip_filename)
    
    end_clip = createEndClip(os.path.join(assetspath, "shortend.png"), "check out the full video on our channel now", end_clip_path)
    print(f"Created end clip: {end_clip}")
    
    # Step 3: Combine main video with end clip
    combined_filename = f"temp_combined_{segment_id}.mp4"
    combined_path = os.path.join("cache", "shorts", combined_filename)
    
    combined_video = combineVideos([nine_sixteen_video, end_clip], combined_path)
    print(f"Combined main video with end clip: {combined_video}")

    # Step 4: Add TikTok-style captions
    temp_captioned_filename = f"temp_captions_{segment_id}.mp4"
    temp_captioned_path = os.path.join("cache", "shorts", temp_captioned_filename)
    
    captioned_video = add_tiktok_captions(combined_video, temp_captioned_path)
    print(f"Added captions to video: {captioned_video}")
    
    # Step 5: Speed up the final video
    final_filename = f"short_9x16_captions_speed{speed_multiplier}x_{segment_id}.mp4"
    final_output_path = os.path.join("cache", "shorts", final_filename)
    
    final_video = speedUpVideo(captioned_video, final_output_path, speed_multiplier)
    print(f"Sped up video by {speed_multiplier}x: {final_video}")
    
    # Clean up temporary files
    temp_files = [temp_9x16_path, end_clip_path, combined_path, temp_captioned_path]
    for temp_file in temp_files:
        if os.path.exists(temp_file) and temp_file != final_video:
            os.remove(temp_file)
            print(f"Cleaned up temporary file: {temp_file}")
    
    # Build a safe, concise YouTube title (<= 100 chars, non-empty)
    def _sanitize(text: str) -> str:
        # Collapse whitespace and remove control chars
        text = re.sub(r"\s+", " ", text or "").strip()
        text = re.sub(r"[\x00-\x1F\x7F]", "", text)
        return text

    def _truncate(text: str, max_len: int = 100) -> str:
        if len(text) <= max_len:
            return text
        # leave room for ellipsis
        cut = max_len - 1
        return text[:cut].rstrip() + "…"

    title_subject = subidea.get("subject", str(subidea)) if isinstance(subidea, dict) else str(subidea)
    raw_title = f"{_sanitize(str(video_idea))} - { _sanitize(str(title_subject)) }"
    safe_title = _truncate(_sanitize(raw_title), 100)
    if not safe_title:
        safe_title = "Cool Short"

    publish_short(
        title=safe_title,
        file_path=final_video,
        assetspath=assetspath,
        base_description="Check out the full video on our channel now!"
    )
import json
import re
import os
import hashlib
import shutil
from gemini import ask_gemini  # Assuming this exists based on context
from buildShot import buildShot
from getTTS import getTTS
from getTimestamps import get_phrase_timestamps
from getImage import getImage
from getAudioLength import getAudioLength
from overlayAudioVideo import overlayAudioVideo
from overWriteFirstSecondsWithLastFrame import overWriteFirstSecondsWithLastFrame
from combineVideos import combineVideos
from concurrent.futures import ThreadPoolExecutor, as_completed

CACHE_DIR = "cache/wholeshot"
os.makedirs(CACHE_DIR, exist_ok=True)

# Extra silence to append to the very end of the whole shot (seconds)
WHOLE_SHOT_END_SILENCE_SECONDS = 0.5

def _cache_path(concept, larger_video):
    key_src = f"{concept}|{larger_video}".encode("utf-8")
    return os.path.join(CACHE_DIR, hashlib.md5(key_src).hexdigest() + ".mp4")

VO_PLAN="""Write a one minute long VO script for the following concept in the context of the larger video.
Concept: {concept}
Larger Video: {larger_video}

Output just a single standalone paragraph, don't use continueing language like "next up".
Explain plainly without overly verbose word choice, but make it interesting. Use examples to help the viewer understand.
This will just be a subsection of the larger video, so don't explain the larger video idea but rather explain JUST the concept.

NEVER EVER START A SENTENCE WITH A ACRONYM"""

MAKE_MEDIA="""Break this script down into shots and build the media for each shot.
You should return an array of shot objects.
Shot objects should be the following:
{{
vo:"the EXACT portion of the full vo that should be aligned with this shot",
media:[
an array of media objects
]
}}
Use media objects to represent all the key concepts in each shot.
Media objects can either be text or images.
Text objects should be the following
{{
text:"just the text",
appearAt:"the EXACT text from the shot's VO that should trigger the text to show"
}}
Image objects should be the following
{{
imageSearch"a search that could be put into google images to find the desired images",
goal:"the goal of the image, what it should convey (to help the editor search and pick the best image)",
caption:"an optional param, a text snippet that will be directly below the image",
appearAt:"the EXACT text from the shot's VO that should trigger the image to show"
}}
Never use graphs or charts for an image.

Full VO to be broken down into shots:
{vo}"""


# Parse the response flexibly - look for JSON in markdown blocks or plain text
def parse_json_response(response):
    # First try to find JSON in markdown code blocks
    json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Try to find JSON array in the response without markdown
    json_match = re.search(r'(\[.*?\])', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # If no JSON found, raise an error
    raise ValueError(f"Could not parse JSON from response: {response}")

def makeWholeShot(concept, larger_video, assetspath: str = "."):
    concept = concept.strip()
    larger_video = larger_video.strip()
    if not concept or not larger_video:
        raise ValueError("makeWholeShot: concept and larger_video cannot be empty.")

    print(f"WholeShot: Processing concept: {concept[:80]!r}...")  # trim for logs

    cache_path = _cache_path(concept, larger_video)
    
    if os.path.exists(cache_path):
        return cache_path

    vo_plan = VO_PLAN.format(concept=concept, larger_video=larger_video)
    vo_script = ask_gemini(vo_plan,model="gemini-2.5-pro")
    
    make_media = MAKE_MEDIA.format(vo=vo_script)
    make_media_response = ask_gemini(make_media,model="gemini-2.5-pro")
    media_plan = parse_json_response(make_media_response)

    
    # Pre-cache all images up front and in parallel so later calls are fast
    # Build unique list of (imageSearch, goal) pairs
    image_jobs = []
    for shot in media_plan:
        for media in shot.get("media", []):
            if "text" in media:
                continue
            search = media.get("imageSearch")
            goal = media.get("goal")
            if not search or not goal:
                continue
            image_jobs.append((search, goal))

    # Deduplicate while preserving order
    unique_image_jobs = list(dict.fromkeys(image_jobs))

    pre_cached_images = {}
    if unique_image_jobs:
        max_workers = min(8, len(unique_image_jobs))

        def _cache_one(job):
            s, g = job
            try:
                path = getImage(s, g)
                return job, path
            except Exception as e:
                print(f"Pre-cache getImage failed for {s!r}: {e}")
                return job, None

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(_cache_one, job) for job in unique_image_jobs]
            for fut in as_completed(futures):
                job, path = fut.result()
                if path:
                    pre_cached_images[job] = path
    SHOT_SWITCH_TIME_PADDING = 0.5

    shot_paths= []

    

    for i in range(len(media_plan)):
        vo_tts = getTTS(media_plan[i]["vo"], voice="Liam", previous_text=media_plan[i-1]["vo"] if i>0 else None)
        media_timestamps_map = get_phrase_timestamps([x["appearAt"] for x in media_plan[i]["media"]], vo_tts)

        clean_media=[]
        for media in media_plan[i]["media"]:
            if "text" in media:
                clean_media.append({
                    "text": media["text"],
                    "appearAt": media_timestamps_map[media["appearAt"]],
                })
            else:
                key = (media.get("imageSearch"), media.get("goal"))
                img_path = pre_cached_images.get(key)
                if not img_path:
                    img_path = getImage(media["imageSearch"], media["goal"])  # fallback (cache miss)
                clean_media.append({
                    "path": img_path,
                    "appearAt": media_timestamps_map[media["appearAt"]],
                })

        # Base duration equals the TTS audio length plus the original 1s hold; add extra end silence only for the final shot
        duration_seconds = getAudioLength(vo_tts) + 1
        if i == len(media_plan) - 1:
            duration_seconds += WHOLE_SHOT_END_SILENCE_SECONDS
        shot_path = buildShot(clean_media, duration_seconds, 
                             font_path=os.path.join(assetspath, "font.ttf"),
                             background_path=os.path.join(assetspath, "background.png"))
        if i == len(media_plan) - 1:
            overlayAudioVideo(shot_path, vo_tts, trim_to_shortest=False)
        else:
            overlayAudioVideo(shot_path, vo_tts)

        if i>0: 
            firstMediaTimestamp = min(media_timestamps_map.values())
            overWriteFirstSecondsWithLastFrame(shot_path, shot_paths[-1], firstMediaTimestamp-SHOT_SWITCH_TIME_PADDING)
        shot_paths.append(shot_path)

    # Generate temporary output path
    temp_output = "temp_output.mp4"
    combineVideos(shot_paths, temp_output)
    
    # Copy to cache
    shutil.copy2(temp_output, cache_path)
    
    # Clean up temporary file
    os.remove(temp_output)

    return cache_pathimport subprocess
import os

def overlayAudioVideo(video_path: str, audio_path: str, trim_to_shortest: bool = True) -> str:
    """Overlay audio directly on video using ffmpeg, overwriting the original video file.
    
    Args:
        video_path: Path to the input video file (will be overwritten)
        audio_path: Path to the input audio file
        trim_to_shortest: If True, output stops at the end of the shorter stream. If False,
            output continues to the longest stream (commonly the video), leaving silence after audio ends.
        
    Returns:
        Path to the video file (same as input)
    """
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video file not found: {video_path}")
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")
    
    # Normalize paths for cross-platform compatibility
    video_path = os.path.normpath(video_path)
    audio_path = os.path.normpath(audio_path)
    
    # Create temporary output file with proper mp4 extension
    temp_output = video_path.replace('.mp4', '_temp.mp4')
    
    try:
        # Overlay TTS audio on the shot video. Resample with timestamp correction
        # to avoid any speed/pitch issues, and make mappings explicit.
        cmd = [
            'ffmpeg',
            '-hide_banner', '-loglevel', 'error',
            '-i', video_path,
            '-i', audio_path,
            '-map', '0:v:0?',          # first input video
            '-map', '1:a:0?',          # second input audio
            '-c:v', 'copy',            # keep video as-is
            '-c:a', 'aac',
            '-b:a', '192k',
            '-ar', '48000',            # standard video sample rate
            '-ac', '2',
            '-af', 'aresample=async=1:first_pts=0',  # fix timestamps & resample
        ]

        if trim_to_shortest:
            cmd.append('-shortest')

        cmd += [
            '-movflags', '+faststart',
            '-f', 'mp4',
            '-y',                      # overwrite
            temp_output
        ]
        
        print(f"Running ffmpeg command: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        
        # Print ffmpeg output for debugging
        if result.stdout:
            print(f"FFmpeg stdout: {result.stdout}")
        if result.stderr:
            print(f"FFmpeg stderr: {result.stderr}")
        
        # Replace original file with the new one
        os.replace(temp_output, video_path)
        
        return video_path
    except subprocess.CalledProcessError as e:
        # Clean up temp file if it exists
        if os.path.exists(temp_output):
            os.remove(temp_output)
        
        # Print detailed error information
        print(f"FFmpeg command failed with exit code: {e.returncode}")
        if e.stdout:
            print(f"FFmpeg stdout: {e.stdout}")
        if e.stderr:
            print(f"FFmpeg stderr: {e.stderr}")
        
        raise RuntimeError(f"Failed to overlay audio on video. Exit code: {e.returncode}, stderr: {e.stderr}")
    except Exception as e:
        # Clean up temp file if it exists
        if os.path.exists(temp_output):
            os.remove(temp_output)
        raise


# import subprocess
# import os

# def overWriteFirstSecondsWithLastFrame(modify_path, source_path, duration):
#     """Overwrite the first duration seconds of modify_path video with the last frame of source_path video.
#     Preserves the original audio from modify_path.
    
#     Args:
#         modify_path: Path to the video file to modify (will be overwritten)
#         source_path: Path to the source video to extract last frame from
#         duration: Duration in seconds to overwrite at the beginning
        
#     Returns:
#         Path to the modified video file (same as modify_path)
#     """
#     if not os.path.exists(modify_path):
#         raise FileNotFoundError(f"Modify video file not found: {modify_path}")
#     if not os.path.exists(source_path):
#         raise FileNotFoundError(f"Source video file not found: {source_path}")
    
#     # Duration bounds checking to prevent timing issues
#     if duration <= 0:
#         print(f"Warning: Duration {duration} is <= 0, skipping overwrite")
#         return modify_path
#     if duration < 0.1:
#         print(f"Warning: Duration {duration} is very small, setting to minimum 0.1s")
#         duration = 0.1
    
#     # Normalize paths
#     modify_path = os.path.normpath(modify_path)
#     source_path = os.path.normpath(source_path)
    
#     # Create temporary output file
#     temp_output = modify_path.replace('.mp4', '_temp.mp4')
    
#     try:
#         # Use ffmpeg to:
#         # 1. Extract the last frame from source video and loop it for duration seconds
#         # 2. Take the rest of the modify video after duration seconds  
#         # 3. Concatenate them together
#         # 4. Keep original audio from modify video
#         cmd = [
#             'ffmpeg',
#             '-sseof', '-1',           # Start from 1 second before end
#             '-i', source_path,        # Source video (for last frame)
#             '-i', modify_path,        # Modify video (for audio and remaining video)
#             '-filter_complex',
#             # Reset PTS on both parts to ensure exact alignment and prevent drift
#             f'[0:v]scale=1920:1080,loop=loop=-1:size=1:start=0,trim=duration={duration},setpts=PTS-STARTPTS[firstpart];'
#             f'[1:v]trim=start={duration},setpts=PTS-STARTPTS[secondpart];'
#             f'[firstpart][secondpart]concat=n=2:v=1:a=0,setpts=PTS-STARTPTS[outv];'
#             f'[1:a]asetpts=PTS-STARTPTS[aout]',
#             '-map', '[outv]',         # Use concatenated video
#             '-map', '[aout]',         # Use original audio with reset PTS
#             '-c:v', 'libx264',        # Video codec
#             '-c:a', 'aac',            # Re-encode audio to apply filter
#             '-avoid_negative_ts', 'make_zero',  # Handle timing precision issues
#             '-y',                     # Overwrite output file
#             temp_output
#         ]
        
#         print(f"Running ffmpeg command: {' '.join(cmd)}")
        
#         result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        
#         # Print ffmpeg output for debugging
#         if result.stdout:
#             print(f"FFmpeg stdout: {result.stdout}")
#         if result.stderr:
#             print(f"FFmpeg stderr: {result.stderr}")
        
#         # Verify the temp output was created successfully
#         if not os.path.exists(temp_output):
#             raise RuntimeError(f"FFmpeg failed to create output file: {temp_output}")
        
#         # Replace original file with the new one
#         os.replace(temp_output, modify_path)
        
#         print(f"Successfully overwrote first {duration}s of {modify_path} with last frame from {source_path}")
#         return modify_path
        
#     except subprocess.CalledProcessError as e:
#         # Clean up temp file if it exists
#         if os.path.exists(temp_output):
#             os.remove(temp_output)
        
#         print(f"FFmpeg command failed with exit code: {e.returncode}")
#         if e.stdout:
#             print(f"FFmpeg stdout: {e.stdout}")
#         if e.stderr:
#             print(f"FFmpeg stderr: {e.stderr}")
        
#         raise RuntimeError(f"Failed to overwrite video. Exit code: {e.returncode}, stderr: {e.stderr}")
#     except Exception as e:
#         # Clean up temp file if it exists
#         if os.path.exists(temp_output):
#             os.remove(temp_output)
#         raise

import subprocess
import os
from typing import Optional
import cv2  # type: ignore
from getLastFrame import getLastFrame

def overWriteFirstSecondsWithLastFrame(modify_path: str, source_path: str, duration: float, fps: Optional[float] = None) -> str:
    """Overwrite the first N frames (duration * fps) of `modify_path` with the last
    frame of `source_path`, using OpenCV for video, while keeping the original
    audio stream untouched (bitstream-copied).

    Args:
        modify_path: Path to the video we will modify in-place.
        source_path: Path to the source video from which we take the last frame.
        duration: Seconds of the start to overwrite.
        fps: Optional override for FPS to compute how many frames to overwrite.

    Returns:
        The same `modify_path` after modification.
    """
    if not os.path.exists(modify_path):
        raise FileNotFoundError(f"Modify video file not found: {modify_path}")
    if not os.path.exists(source_path):
        raise FileNotFoundError(f"Source video file not found: {source_path}")

    if duration <= 0:
        return modify_path

    modify_path = os.path.normpath(modify_path)
    source_path = os.path.normpath(source_path)

    # Prepare paths
    root, ext = os.path.splitext(modify_path)
    temp_video_noaudio = f"{root}.__temp_noaudio__{ext or '.mp4'}"
    temp_output = f"{root}.__temp__{ext or '.mp4'}"

    last_frame_png: Optional[str] = None
    cap = None
    writer = None
    try:
        # Open the video to modify to read metadata and frames
        cap = cv2.VideoCapture(modify_path)
        if not cap.isOpened():
            raise RuntimeError("cv2.VideoCapture failed to open modify_path")

        video_fps = cap.get(cv2.CAP_PROP_FPS)
        if not video_fps or video_fps <= 0:
            # Fall back to provided fps or a safe default
            video_fps = fps if fps and fps > 0 else 30.0

        total_frames_float = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        total_frames = int(total_frames_float) if total_frames_float and total_frames_float > 0 else None
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        if width <= 0 or height <= 0:
            raise RuntimeError("Could not determine video dimensions from modify_path")

        # How many frames to overwrite at the start
        frames_to_overwrite = int(round(duration * float(video_fps)))
        if total_frames is not None:
            frames_to_overwrite = max(0, min(frames_to_overwrite, total_frames))
        if frames_to_overwrite <= 0:
            # Nothing to do
            return modify_path

        # Load the last frame from the source video
        last_frame_png = getLastFrame(source_path)
        hold_img = cv2.imread(last_frame_png, cv2.IMREAD_COLOR)
        if hold_img is None:
            raise RuntimeError("cv2.imread failed to load the last frame PNG")
        if hold_img.shape[1] != width or hold_img.shape[0] != height:
            hold_img = cv2.resize(hold_img, (width, height), interpolation=cv2.INTER_AREA)

        # Write a new temporary video (no audio) using OpenCV
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        writer = cv2.VideoWriter(temp_video_noaudio, fourcc, video_fps, (width, height))
        if not writer.isOpened():
            raise RuntimeError("cv2.VideoWriter failed to open temp video path")

        # 1) Write the held last-frame for the required number of frames
        for _ in range(frames_to_overwrite):
            writer.write(hold_img)

        # 2) Skip the first N frames in the original video and write the rest
        # Try seeking; if it fails, fall back to reading-and-discarding
        seek_ok = cap.set(cv2.CAP_PROP_POS_FRAMES, frames_to_overwrite)
        if not seek_ok:
            # Fallback: read and discard frames_to_overwrite frames
            dropped = 0
            while dropped < frames_to_overwrite:
                ret, _ = cap.read()
                if not ret:
                    break
                dropped += 1

        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if frame is None:
                break
            # Ensure frame matches output size (just in case)
            if frame.shape[1] != width or frame.shape[0] != height:
                frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)
            writer.write(frame)

        # Close writer to flush file
        writer.release()
        writer = None
        cap.release()
        cap = None

        # Now mux original audio (untouched) with the new video, re-encoding
        # the video to H.264 to keep codec compatibility across shots.
        cmd = [
            "ffmpeg",
            "-hide_banner", "-loglevel", "error",
            "-y",
            "-i", temp_video_noaudio,
            "-i", modify_path,
            "-map", "0:v:0",
            "-map", "1:a:0?",
            "-c:v", "libx264",
            "-pix_fmt", "yuv420p",
            "-r", f"{video_fps}",
            "-vsync", "cfr",
            "-preset", "medium",
            "-movflags", "+faststart",
            "-c:a", "copy",
            temp_output,
        ]
        subprocess.run(cmd, check=True, text=True, capture_output=True)
        if not os.path.exists(temp_output):
            raise RuntimeError("FFmpeg did not create the expected output.")

        # Replace original with the muxed output
        os.replace(temp_output, modify_path)

        # Clean up temp video-without-audio
        if os.path.exists(temp_video_noaudio):
            try:
                os.remove(temp_video_noaudio)
            except OSError:
                pass

        return modify_path

    except subprocess.CalledProcessError as e:
        # Clean temp files on failure
        for p in (temp_output, temp_video_noaudio):
            try:
                if p and os.path.exists(p):
                    os.remove(p)
            except OSError:
                pass
        raise RuntimeError(
            f"FFmpeg failed (exit {e.returncode}).\nSTDERR:\n{e.stderr or ''}"
        )
    finally:
        # Release OpenCV resources if still open
        if writer is not None:
            try:
                writer.release()
            except Exception:
                pass
        if cap is not None:
            try:
                cap.release()
            except Exception:
                pass
        # Clean PNG last frame
        if last_frame_png and os.path.exists(last_frame_png):
            try:
                os.remove(last_frame_png)
            except OSError:
                pass


from buildShot import buildShot
from getTTS import getTTS
from getTimestamps import get_phrase_timestamps
from getImage import getImage
from getAudioLength import getAudioLength
from overlayAudioVideo import overlayAudioVideo
from overWriteFirstSecondsWithLastFrame import overWriteFirstSecondsWithLastFrame
from combineVideos import combineVideos
from getLastFrame import getLastFrame
from getSubideas import getSubideas
from makeWholeShot import makeWholeShot
from makeAllIdeasImage import zoomintoidea, makeAllIdeasImage
from getMetadata import getMetadata
from upload_video import publish_simple
from makeAndUploadShort import makeAndUploadShort
import os
import re
import requests

def runit(assetspath):
    def check_ideas_and_notify():
        """Check if next_ideas.txt has fewer than 5 ideas and send Discord webhook if needed."""
        next_ideas_file = os.path.join(assetspath, "next_ideas.txt")
        webhook_url = os.getenv('DISCORD_WEBHOOK')
        
        if not webhook_url:
            return  # No webhook configured, skip notification
            
        if os.path.exists(next_ideas_file):
            with open(next_ideas_file, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f.readlines() if line.strip()]
            
            if len(lines) < 5:
                try:
                    payload = {
                        "content": f"⚠️ **Low Ideas Alert!** Only {len(lines)} ideas remaining in next_ideas.txt. Please add more ideas!"
                    }
                    response = requests.post(webhook_url, json=payload)
                    response.raise_for_status()
                    print(f"Discord notification sent: {len(lines)} ideas remaining")
                except Exception as e:
                    print(f"Failed to send Discord notification: {e}")

    def update_default_title():
        next_ideas_file = os.path.join(assetspath, "next_ideas.txt")
        done_ideas_file = os.path.join(assetspath, "done_ideas.txt")
        
        if os.path.exists(next_ideas_file):
            with open(next_ideas_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            if lines:
                # Get first line and strip whitespace
                first_line = lines[0].strip()
                
                # Write remaining lines back to next_ideas.txt
                with open(next_ideas_file, 'w', encoding='utf-8') as f:
                    f.writelines(lines[1:])
                
                # Append first line to done_ideas.txt
                with open(done_ideas_file, 'a', encoding='utf-8') as f:
                    f.write(first_line + '\n')
                
                return first_line
            else:
                raise Exception("No ideas available in next_ideas.txt")
        else:
            raise Exception("next_ideas.txt file not found")

    video_idea = update_default_title()
    check_ideas_and_notify()

    subideas = getSubideas(video_idea)
    makeAllIdeasImage(subideas, output_path=os.path.join(assetspath, "thumbnail.png"), size=(1920, 1080), background_path=os.path.join(assetspath, "background.png"), font_path=os.path.join(assetspath, "font.ttf"))

    # Ensure zoom cache directory exists
    zoom_dir = os.path.join("cache", "zooms")
    os.makedirs(zoom_dir, exist_ok=True)

    def _slugify(text: str) -> str:
        return re.sub(r"[^A-Za-z0-9_-]+", "_", text).strip("_")

    # Build sequence: [zoom_to_0, whole_0, zoom_to_1, whole_1, ...]
    segments = []
    for idx, sub in enumerate(subideas):
        subject = sub.get("subject", f"item_{idx}")
        zoom_out = os.path.join(zoom_dir, f"{idx:02d}_{_slugify(subject)}.mp4")
        # Build zoom transition into this subidea from the grid of all items
        zoom_path = zoomintoidea(subideas, idx, zoom_out, size=(1920, 1080), background_path=os.path.join(assetspath, "background.png"), font_path=os.path.join(assetspath, "font.ttf"))
        segments.append(zoom_path)

        # Build the subidea's full shot
        whole_path = makeWholeShot(subject, video_idea, assetspath)
        segments.append(whole_path)

    # Stitch full program
    final_output = os.path.join(assetspath, "final.mp4")
    combineVideos(segments, final_output)


    # --- Generate AI description and keywords, then upload ---
    description, keywords_csv = getMetadata(video_idea, subideas)

    thumb_path = os.path.join(assetspath, "thumbnail.png") if os.path.exists(os.path.join(assetspath, "thumbnail.png")) else None

    try:
        watch_url = publish_simple(
            title=video_idea,
            file_path=final_output,
            description=description,
            assetspath=assetspath,
            thumbnail_path=thumb_path,
            category="27",  # Education
            keywords=keywords_csv
        )
        print(f"Uploaded: {watch_url}")
    except Exception as e:
        print(f"Upload failed: {e}")

    for i in range(0, min(8, len(segments)), 2):
        makeAndUploadShort(segments[i+1], video_idea, subideas[i//2], assetspath)  #!/usr/bin/python
#!/usr/bin/env python3
"""
Auto-upload to YouTube and publish immediately as public.

Required scopes:
  • https://www.googleapis.com/auth/youtube.upload
"""
import http.client as httplib
import httplib2
import json
import os, random, shutil, subprocess, sys, time
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
import argparse
from types import SimpleNamespace

# ─── Retry / upload constants ──────────────────────────────────────────────────
httplib2.RETRIES = 1
MAX_RETRIES = 10
RETRIABLE_EXCEPTIONS = (
    httplib2.HttpLib2Error, IOError, httplib.NotConnected,
    httplib.IncompleteRead, httplib.ImproperConnectionState,
    httplib.CannotSendRequest, httplib.CannotSendHeader,
    httplib.ResponseNotReady, httplib.BadStatusLine,
)
RETRIABLE_STATUS_CODES = [500, 502, 503, 504]

# ─── OAuth / API constants ─────────────────────────────────────────────────────
UPLOAD_SCOPE   = "https://www.googleapis.com/auth/youtube.upload"
SCOPES = [UPLOAD_SCOPE]
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION      = "v3"
VALID_PRIVACY_STATUSES = ("public", "private", "unlisted")

# ───────────────────────────────────────────────────────────────────────────────
def get_authenticated_service(assetspath):
    """
    Re-auth if needed; cache user OAuth tokens in a separate token file
    """
    if not assetspath:
        raise ValueError("assetspath is required for authentication")
    
    creds = None
    
    # Use authentication files from assetspath
    token_file = os.path.join(assetspath, "youtube_token.json")
    client_secrets_file = os.path.join(assetspath, "client_secret.json")
    
    scopes = SCOPES        # instead of [UPLOAD_SCOPE, READONLY_SCOPE]

    # Load existing credentials from token file
    if os.path.exists(token_file):
        creds = Credentials.from_authorized_user_file(token_file, scopes)

    # If there are no valid credentials available, get new ones
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(Request())
            except Exception as e:
                print(f"Token refresh failed: {e}")
                creds = None
        
        # If refresh failed or no valid creds, run the OAuth flow
        if not creds:
            if not os.path.exists(client_secrets_file):
                raise FileNotFoundError(f"Client secrets file not found: {client_secrets_file}")
            
            flow = InstalledAppFlow.from_client_secrets_file(client_secrets_file, scopes)
            creds = flow.run_local_server(port=0)
        
        # Save the credentials for the next run
        os.makedirs(os.path.dirname(token_file), exist_ok=True)
        with open(token_file, "w") as token:
            token.write(creds.to_json())

    service = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, credentials=creds)
    return service


def initialize_upload(youtube, options):
    """
    Uploads the file as **public**, returns the new video_id.
    """
    tags = options.keywords.split(",") if options.keywords else None

    body = dict(
        snippet=dict(
            title       = options.title,
            description = options.description,
            tags        = tags,
            categoryId  = options.category,
        ),
        status=dict(
            privacyStatus="public",            ### CHANGED: always public immediately
            selfDeclaredMadeForKids=False,
        ),
    )

    insert_request = youtube.videos().insert(
        part=",".join(body.keys()),
        body=body,
        media_body=MediaFileUpload(options.file, chunksize=-1, resumable=True),
    )
    return resumable_upload(insert_request)      ### CHANGED: now returns video_id


def resumable_upload(insert_request):
    """
    Standard exponential-backoff upload. Returns the video_id when done.
    """
    response, error, retry = None, None, 0
    
    while response is None:
        try:
            status, response = insert_request.next_chunk()
            
            if response is not None and "id" in response:
                vid = response["id"]
                return vid
            else:
                sys.exit(f"Unexpected upload response: {response}")
                
        except HttpError as e:
            if e.resp.status in RETRIABLE_STATUS_CODES:
                error = f"A retriable HTTP error {e.resp.status}: {e.content}"
            else:
                raise
        except RETRIABLE_EXCEPTIONS as e:
            error = f"A retriable error occurred: {e}"

        if error:
            retry += 1
            if retry > MAX_RETRIES:
                sys.exit("Giving up.")
            sleep = random.random() * (2 ** retry)
            time.sleep(sleep)
            error = None  # Reset error for next iteration


# ─── Thumbnail setting logic ──────────────────────────────────────────────────
def set_thumbnail(youtube, video_id, thumbnail_path):
    """
    Set the thumbnail for a YouTube video.
    
    Args:
        youtube: Authenticated YouTube service
        video_id: YouTube video ID
        thumbnail_path: Path to the thumbnail image file
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        print(f"🖼️  Setting thumbnail for video {video_id}")
        
        youtube.thumbnails().set(
            videoId=video_id,
            media_body=MediaFileUpload(thumbnail_path)
        ).execute()
        
        print(f"✅ Thumbnail set successfully")
        return True
        
    except Exception as e:
        error_msg = str(e)
        if "doesn't have permissions to upload and set custom video thumbnails" in error_msg:
            print(f"⚠️  Thumbnail upload skipped: Your YouTube channel needs to be verified to upload custom thumbnails")
            print(f"   You can verify your channel at: https://www.youtube.com/verify")
            print(f"   Once verified, you can manually set the thumbnail from: {thumbnail_path}")
        else:
            print(f"❌ Failed to set thumbnail: {e}")
        return False



def publish_simple(title: str, file_path: str, description: str, assetspath: str,
                   thumbnail_path: str = None, category: str = "22", keywords: str = "") -> str:
    """
    Convenience one-shot publish:
      1) auth
      2) upload as *public*
      3) (optional) set custom thumbnail
    Returns the watch URL.
    """
    yt = get_authenticated_service(assetspath)
    opts = SimpleNamespace(
        title=title,
        description=description,
        category=category,
        keywords=keywords,
        file=file_path,
    )
    video_id = initialize_upload(yt, opts)
    
    # Try to set thumbnail if provided, but don't fail the upload if it doesn't work
    if thumbnail_path:
        thumbnail_success = set_thumbnail(yt, video_id, thumbnail_path)
        if not thumbnail_success:
            print("⚠️  Continuing with upload despite thumbnail failure...")
    
    print(f"Video {video_id} published as public: https://www.youtube.com/watch?v={video_id}")
    return f"https://www.youtube.com/watch?v={video_id}"

def _probe_short_file(file_path: str) -> None:
    """
    Best-effort sanity checks for Shorts:
      • Warn if duration > 180s (3 min)
      • Warn if not vertical (height <= width)
    Uses ffprobe if available; otherwise no-ops.
    """
    try:
        if not shutil.which("ffprobe"):
            print("ℹ️  ffprobe not found; skipping Shorts validation.")
            return
        proc = subprocess.run(
            ["ffprobe", "-v", "error", "-show_streams", "-show_format", "-print_format", "json", file_path],
            capture_output=True, text=True, check=True
        )
        info = json.loads(proc.stdout)
        v = next((s for s in info.get("streams", []) if s.get("codec_type") == "video"), {})
        width, height = int(v.get("width", 0)), int(v.get("height", 0))
        # duration may be on stream or format; take whichever is present
        dur_s = float(v.get("duration") or info.get("format", {}).get("duration") or 0.0)

        if dur_s > 180.5:
            print(f"⚠️  Duration ~{dur_s:.1f}s > 180s. Shorts are up to 3 minutes.")
        if height <= width:
            print(f"⚠️  Video is not vertical ({width}x{height}). Shorts work best vertical.")

    except Exception as e:
        print(f"ℹ️  Shorts validation skipped: {e}")


def publish_short(
    title: str,
    file_path: str,
    assetspath: str,
    base_description: str = "",
    full_video_url: str = "",
    *,
    timestamp_seconds: int | None = None,   # if set, appends &t=...s to full_video_url
    include_hashtag_shorts: bool = False,
    thumbnail_path: str | None = None,
    category: str = "22",
    keywords: str = ""
) -> str:
    """
    Upload a YouTube Short (<=3 min, vertical) as public immediately.
    Returns the watch URL.

    Linking to full video:
      • Puts a clickable YouTube link at the very top of the Short's description.
      • For the in-app Shorts "Related video" pill, set it later in YouTube Studio.
        (Not exposed via the Data API.)
    """
    # Build description with the full video URL at the top.
    desc_lines = []
    if full_video_url:
        link = full_video_url.strip()
        if timestamp_seconds is not None and timestamp_seconds >= 0:
            # Add &t=...s to either /watch?v=... or youtu.be/... forms.
            if "watch?v=" in link:
                sep = "&" if "?" in link else "?"
                link = f"{link}{sep}t={int(timestamp_seconds)}s"
            elif "youtu.be/" in link:
                sep = "?" if "?" not in link else "&"
                link = f"{link}{sep}t={int(timestamp_seconds)}s"
        desc_lines.append(f"Full video: {link}")
    if base_description:
        desc_lines.append(base_description)
    if include_hashtag_shorts:
        desc_lines.append("#Shorts")
    final_description = "\n\n".join([s for s in desc_lines if s]).strip()

    # Optional sanity checks for Shorts constraints
    _probe_short_file(file_path)

    yt = get_authenticated_service(assetspath)
    opts = SimpleNamespace(
        title=title,
        description=final_description,
        category=category,
        keywords=keywords,
        file=file_path,
    )

    vid = initialize_upload(yt, opts)

    if thumbnail_path:
        try:
            set_thumbnail(yt, vid, thumbnail_path)
        except Exception as e:
            print(f"⚠️  Thumbnail step failed/skipped: {e}")

    url = f"https://www.youtube.com/watch?v={vid}"
    print(f"✅ Short published: {url}")
    return url


# ─── CLI boilerplate ───────────────────────────────────────────────────────────
if __name__ == "__main__":
    # publish_simple(
    #     title="Test Title",
    #     file_path="output_overlay.mp4",
    #     description="Test Description",
    #     thumbnail_path="cache/thumbnails/28f4eb7f861db4adcdbe2d9d72608f38.png",
    #     category="27",
    #     keywords=""
    # )

    # publish_short(
    #     title="No-Fly List",
    #     file_path="cache/shorts/short_9x16_captions_speed2x_421d97c034d6e00acac7e9be9c6ca3a7.mp4",
    #     base_description="Check out the full video on our channel now!",
    # )


    # parser = argparse.ArgumentParser()
    # parser.add_argument("--file", help="Video file to upload (optional for auth-only)")
    # parser.add_argument("--title", default="Test Title", help="Video title")
    # parser.add_argument("--description", default="Test Description")
    # parser.add_argument("--category", default="22", help="Numeric video category")
    # parser.add_argument("--keywords", default="", help="Comma-separated keywords")
    # args = parser.parse_args()

    # # Just do auth flow if no file specified
    # if not args.file:
    #     print("No file specified - running auth flow only...")
    #     get_authenticated_service()
    #     print("Authentication complete! .json file should now be available.")
    #     sys.exit(0)

    # if not os.path.exists(args.file):
    #     sys.exit("Invalid --file path")
    
    # yt = get_authenticated_service()

    # try:
    #     vid = initialize_upload(yt, args)
    #     poll_and_publish(yt, vid)
        
    # except HttpError as e:
    #     print(f"HTTP error occurred: {e.resp.status} - {e.content}")